{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:49:48.833107Z",
     "start_time": "2021-08-03T17:49:42.914479Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:49:50.204271Z",
     "start_time": "2021-08-03T17:49:50.199341Z"
    }
   },
   "outputs": [],
   "source": [
    "#  predition TI of leading time at 24 hours\n",
    "pre_seq = 4\n",
    "batch_size = 128\n",
    "epochs = 128\n",
    "min_val_loss = 100\n",
    "model_name = 'SAF_Net.pkl'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:49:51.275600Z",
     "start_time": "2021-08-03T17:49:50.943198Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/CMA_train_'+str(pre_seq*6)+'h.csv', header=None)\n",
    "test= pd.read_csv('./data/CMA_test_'+str(pre_seq*6)+'h.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:49:51.840777Z",
     "start_time": "2021-08-03T17:49:51.823629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8406, 101), (2747, 101))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:49:52.363694Z",
     "start_time": "2021-08-03T17:49:52.335495Z"
    }
   },
   "outputs": [],
   "source": [
    "CLIPER_feature =  pd.concat((train, test), axis=0)\n",
    "CLIPER_feature.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:49:52.978471Z",
     "start_time": "2021-08-03T17:49:52.920043Z"
    }
   },
   "outputs": [],
   "source": [
    "X_wide_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X_wide = X_wide_scaler.fit_transform(CLIPER_feature.iloc[:, 5:])\n",
    "X_wide_train = X_wide[0: train.shape[0], :]\n",
    "\n",
    "y = y_scaler.fit_transform(CLIPER_feature.loc[:, 3].values.reshape(-1, 1))\n",
    "y_train = y[0: train.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:49:53.436650Z",
     "start_time": "2021-08-03T17:49:53.431475Z"
    }
   },
   "outputs": [],
   "source": [
    "# now 6 hours ago  12 hours ago  18 hour ago\n",
    "ahead_times = [0,1,2,3]\n",
    "\n",
    "pressures = [1000, 750, 500, 250]\n",
    "\n",
    "sequential_reanalysis_u_list = []\n",
    "reanalysis_u_test_dict = {}\n",
    "X_deep_u_scaler_dict = {}\n",
    "\n",
    "sequential_reanalysis_v_list = []\n",
    "reanalysis_v_test_dict = {}\n",
    "X_deep_v_scaler_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:50:28.798167Z",
     "start_time": "2021-08-03T17:49:53.943725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahead_time: 0 (8406, 1, 4, 31, 31, 1)\n",
      "ahead_time: 1 (8406, 1, 4, 31, 31, 1)\n",
      "ahead_time: 2 (8406, 1, 4, 31, 31, 1)\n",
      "ahead_time: 3 (8406, 1, 4, 31, 31, 1)\n"
     ]
    }
   ],
   "source": [
    "reanalysis_type = 'u'\n",
    "for ahead_time in ahead_times:\n",
    "    reanalysis_list = []\n",
    "    for pressure in pressures:\n",
    "        folder = None\n",
    "        if ahead_time == 0:\n",
    "            folder = reanalysis_type\n",
    "        else:\n",
    "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
    "            \n",
    "        train_reanalysis_csv = pd.read_csv('./data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_train_31_31.csv', header=None)\n",
    "        test_reanalysis_csv = pd.read_csv('./data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_test_31_31.csv', header=None)\n",
    "\n",
    "        train_reanalysis = train_reanalysis_csv[train_reanalysis_csv[0].isin(train[0].unique())]\n",
    "        test_reanalysis = test_reanalysis_csv[test_reanalysis_csv[0].isin(test[0].unique())]\n",
    "        reanalysis_u_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
    "        \n",
    "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
    "        reanalysis.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "        X_deep_u_scaler_dict[scaler_name] = MinMaxScaler()\n",
    "        \n",
    "        # 5:end is the 31*31 u component wind speed\n",
    "        X_deep = X_deep_u_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
    "        \n",
    "        # (batch, type, channel, height, widht, time) here type is u\n",
    "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 31, 31, 1)\n",
    "        reanalysis_list.append(X_deep_final)\n",
    "\n",
    "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
    "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
    "    sequential_reanalysis_u_list.append(X_deep_temp)\n",
    "\n",
    "X_deep_u_train = np.concatenate(sequential_reanalysis_u_list, axis=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:51:13.975396Z",
     "start_time": "2021-08-03T17:50:28.937634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahead_time: 0 (8406, 1, 4, 31, 31, 1)\n",
      "ahead_time: 1 (8406, 1, 4, 31, 31, 1)\n",
      "ahead_time: 2 (8406, 1, 4, 31, 31, 1)\n",
      "ahead_time: 3 (8406, 1, 4, 31, 31, 1)\n"
     ]
    }
   ],
   "source": [
    "reanalysis_type = 'v'\n",
    "for ahead_time in ahead_times:\n",
    "    reanalysis_list = []\n",
    "    for pressure in pressures:\n",
    "        folder = None\n",
    "        if ahead_time == 0:\n",
    "            folder = reanalysis_type\n",
    "        else:\n",
    "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
    "\n",
    "        train_reanalysis_csv = pd.read_csv('./data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_train_31_31.csv', header=None)\n",
    "        test_reanalysis_csv = pd.read_csv('./data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_test_31_31.csv', header=None)\n",
    "\n",
    "        train_reanalysis = train_reanalysis_csv[train_reanalysis_csv[0].isin(train[0].unique())]\n",
    "        test_reanalysis = test_reanalysis_csv[test_reanalysis_csv[0].isin(test[0].unique())]\n",
    "        reanalysis_v_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
    "\n",
    "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
    "        reanalysis.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "        X_deep_v_scaler_dict[scaler_name] = MinMaxScaler()\n",
    "        \n",
    "        # 5:end is the 31*31 v component wind speed\n",
    "        X_deep = X_deep_v_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
    "        \n",
    "        # (batch, type, channel, height, widht, time) here type is v\n",
    "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 31, 31, 1)\n",
    "        reanalysis_list.append(X_deep_final)\n",
    "        \n",
    "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
    "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
    "    sequential_reanalysis_v_list.append(X_deep_temp)\n",
    "\n",
    "X_deep_v_train = np.concatenate(sequential_reanalysis_v_list, axis=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:51:14.597078Z",
     "start_time": "2021-08-03T17:51:13.976852Z"
    }
   },
   "outputs": [],
   "source": [
    "X_deep_train = np.concatenate((X_deep_u_train, X_deep_v_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:51:14.601402Z",
     "start_time": "2021-08-03T17:51:14.598467Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8406, 96), (8406, 2, 4, 31, 31, 4))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_train.shape, X_deep_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:51:14.651065Z",
     "start_time": "2021-08-03T17:51:14.602369Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainLoader(Data.Dataset):\n",
    "    def __init__(self, X_wide_train, X_deep_train, y_train):\n",
    "        self.X_wide_train = X_wide_train\n",
    "        self.X_deep_train = X_deep_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return [self.X_wide_train[index], self.X_deep_train[index]], self.y_train[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_wide_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:51:14.772160Z",
     "start_time": "2021-08-03T17:51:14.653360Z"
    }
   },
   "outputs": [],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:51:14.878030Z",
     "start_time": "2021-08-03T17:51:14.774372Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7565, 841)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_index), len(val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:51:15.791749Z",
     "start_time": "2021-08-03T17:51:14.881554Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:51:15.884432Z",
     "start_time": "2021-08-03T17:51:15.794510Z"
    }
   },
   "outputs": [],
   "source": [
    "val_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAF-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:51:16.432473Z",
     "start_time": "2021-08-03T17:51:15.885705Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # spatial attention\n",
    "        self.att_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.att_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.att_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        # cross\n",
    "        self.cross_unit = nn.Parameter(data=torch.ones(len(ahead_times), 6))\n",
    "        # fuse\n",
    "        self.fuse_unit = nn.Parameter(data=torch.ones(len(ahead_times), 4))\n",
    "        \n",
    "        # deep\n",
    "        self.conv1 = nn.Conv2d(4, 64, kernel_size=3, padding=(1, 1))\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=(1, 1))\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=(1, 1))\n",
    "        self.fc1 = nn.Linear(256 * 3 * 3, 128)\n",
    "        self.fc2 = nn.Linear(96 + 128 * len(ahead_times), 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, wide, deep):\n",
    "        seq_list = []\n",
    "        for i in range(len(ahead_times)):\n",
    "            \n",
    "            deep_u = deep[:,0,:,:,:,i]\n",
    "            deep_v = deep[:,1,:,:,:,i]\n",
    "            \n",
    "            # split 1\n",
    "            deep_u = self.pool(F.relu(self.conv1(deep_u)))\n",
    "            deep_u = self.att_block_1(deep_u) * deep_u\n",
    "            \n",
    "            deep_v = self.pool(F.relu(self.conv1(deep_v)))\n",
    "            deep_v = self.att_block_1(deep_v) * deep_v\n",
    "            \n",
    "            # fuse 1\n",
    "            time_seq_1 = self.cross_unit[i][0]/(self.cross_unit[i][0]+self.cross_unit[i][1]) * deep_u +\\\n",
    "            self.cross_unit[i][1]/(self.cross_unit[i][0]+self.cross_unit[i][1])  * deep_v\n",
    "            time_seq_1 = self.pool(F.relu(self.conv2(time_seq_1)))\n",
    "            time_seq_1 = self.att_block_2(time_seq_1) * time_seq_1\n",
    "            \n",
    "            # split 2\n",
    "            deep_u = self.pool(F.relu(self.conv2(deep_u)))\n",
    "            deep_u = self.att_block_2(deep_u) * deep_u\n",
    "            \n",
    "            deep_v = self.pool(F.relu(self.conv2(deep_v)))\n",
    "            deep_v = self.att_block_2(deep_v) * deep_v\n",
    "            \n",
    "            # fuse 2\n",
    "            time_seq_2 = self.cross_unit[i][2]/(self.cross_unit[i][2]+self.cross_unit[i][3]) * deep_u +\\\n",
    "            self.cross_unit[i][3]/(self.cross_unit[i][2]+self.cross_unit[i][3]) * deep_v\n",
    "            time_seq_2 = self.fuse_unit[i][0]/(self.fuse_unit[i][0]+self.fuse_unit[i][1]) * time_seq_1+\\\n",
    "            self.fuse_unit[i][1]/(self.fuse_unit[i][0]+self.fuse_unit[i][1]) * time_seq_2\n",
    "            time_seq_2 = self.pool(F.relu(self.conv3(time_seq_2)))\n",
    "            time_seq_2 = self.att_block_3(time_seq_2) * time_seq_2\n",
    "            \n",
    "            # split 3\n",
    "            deep_u = self.pool(F.relu(self.conv3(deep_u)))\n",
    "            deep_u = self.att_block_3(deep_u) * deep_u\n",
    "            \n",
    "            deep_v = self.pool(F.relu(self.conv3(deep_v)))\n",
    "            deep_v = self.att_block_3(deep_v) * deep_v\n",
    "            \n",
    "            # fuse 3\n",
    "            time_seq = self.cross_unit[i][4]/(self.cross_unit[i][4]+self.cross_unit[i][5]) * deep_u +\\\n",
    "            self.cross_unit[i][5]/(self.cross_unit[i][4]+self.cross_unit[i][5]) * deep_v\n",
    "            time_seq = self.fuse_unit[i][2]/(self.fuse_unit[i][2]+self.fuse_unit[i][3]) * time_seq_2+\\\n",
    "            self.fuse_unit[i][3]/(self.fuse_unit[i][2]+self.fuse_unit[i][3]) * time_seq\n",
    "            time_seq = self.att_block_3(time_seq) * time_seq\n",
    "            \n",
    "            time_seq = time_seq.view(-1, 256 * 3 * 3)\n",
    "            time_seq = self.fc1(time_seq)\n",
    "            seq_list.append(time_seq)\n",
    "        wide = wide.view(-1, 96)\n",
    "        wide_n_deep = torch.cat((wide, seq_list[0]),1)\n",
    "        if len(ahead_times) > 1:\n",
    "            for i in range(1, len(ahead_times)):\n",
    "                wide_n_deep = torch.cat((wide_n_deep, seq_list[i]),1)\n",
    "        wide_n_deep = F.relu(self.fc2(wide_n_deep))\n",
    "        wide_n_deep = F.relu(self.fc3(wide_n_deep))\n",
    "        return wide_n_deep\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:51:27.978297Z",
     "start_time": "2021-08-03T17:51:16.434680Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:51:27.986410Z",
     "start_time": "2021-08-03T17:51:27.981101Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs [1/128] train_loss: 6.33109 val_loss: 0.56027\n",
      "epochs [2/128] train_loss: 4.11981 val_loss: 0.46108\n",
      "epochs [3/128] train_loss: 3.58734 val_loss: 0.46666\n",
      "epochs [4/128] train_loss: 3.33584 val_loss: 0.40838\n",
      "epochs [5/128] train_loss: 3.15667 val_loss: 0.33694\n",
      "epochs [6/128] train_loss: 2.92000 val_loss: 0.34625\n",
      "epochs [7/128] train_loss: 2.88240 val_loss: 0.34137\n",
      "epochs [8/128] train_loss: 2.78190 val_loss: 0.32344\n",
      "epochs [9/128] train_loss: 2.64966 val_loss: 0.31312\n",
      "epochs [10/128] train_loss: 2.43837 val_loss: 0.28258\n",
      "epochs [11/128] train_loss: 2.44322 val_loss: 0.28940\n",
      "epochs [12/128] train_loss: 2.34409 val_loss: 0.27879\n",
      "epochs [13/128] train_loss: 2.16938 val_loss: 0.23127\n",
      "epochs [14/128] train_loss: 2.11928 val_loss: 0.27682\n",
      "epochs [15/128] train_loss: 2.13896 val_loss: 0.24279\n",
      "epochs [16/128] train_loss: 1.93012 val_loss: 0.21636\n",
      "epochs [17/128] train_loss: 1.86152 val_loss: 0.20380\n",
      "epochs [18/128] train_loss: 1.83772 val_loss: 0.20506\n",
      "epochs [19/128] train_loss: 1.71140 val_loss: 0.19768\n",
      "epochs [20/128] train_loss: 1.73049 val_loss: 0.22604\n",
      "epochs [21/128] train_loss: 1.74561 val_loss: 0.25153\n",
      "epochs [22/128] train_loss: 1.71180 val_loss: 0.18634\n",
      "epochs [23/128] train_loss: 1.55675 val_loss: 0.18077\n",
      "epochs [24/128] train_loss: 1.55674 val_loss: 0.18555\n",
      "epochs [25/128] train_loss: 1.45332 val_loss: 0.16177\n",
      "epochs [26/128] train_loss: 1.41708 val_loss: 0.18980\n",
      "epochs [27/128] train_loss: 1.41082 val_loss: 0.20691\n",
      "epochs [28/128] train_loss: 1.39406 val_loss: 0.22147\n",
      "epochs [29/128] train_loss: 1.36129 val_loss: 0.23099\n",
      "epochs [30/128] train_loss: 1.37530 val_loss: 0.14164\n",
      "epochs [31/128] train_loss: 1.25112 val_loss: 0.16626\n",
      "epochs [32/128] train_loss: 1.23812 val_loss: 0.17003\n",
      "epochs [33/128] train_loss: 1.22192 val_loss: 0.13171\n",
      "epochs [34/128] train_loss: 1.16371 val_loss: 0.13013\n",
      "epochs [35/128] train_loss: 1.12536 val_loss: 0.17284\n",
      "epochs [36/128] train_loss: 1.11663 val_loss: 0.13871\n",
      "epochs [37/128] train_loss: 1.09945 val_loss: 0.14730\n",
      "epochs [38/128] train_loss: 1.17590 val_loss: 0.14693\n",
      "epochs [39/128] train_loss: 1.14499 val_loss: 0.13076\n",
      "epochs [40/128] train_loss: 1.08618 val_loss: 0.12319\n",
      "epochs [41/128] train_loss: 1.11279 val_loss: 0.16763\n",
      "epochs [42/128] train_loss: 1.10320 val_loss: 0.11650\n",
      "epochs [43/128] train_loss: 1.06723 val_loss: 0.12778\n",
      "epochs [44/128] train_loss: 1.02949 val_loss: 0.16628\n",
      "epochs [45/128] train_loss: 1.14019 val_loss: 0.14161\n",
      "epochs [46/128] train_loss: 1.01402 val_loss: 0.11733\n",
      "epochs [47/128] train_loss: 1.01750 val_loss: 0.16211\n",
      "epochs [48/128] train_loss: 1.02329 val_loss: 0.11525\n",
      "epochs [49/128] train_loss: 0.93223 val_loss: 0.12151\n",
      "epochs [50/128] train_loss: 0.96715 val_loss: 0.12723\n",
      "epochs [51/128] train_loss: 0.96089 val_loss: 0.12546\n",
      "epochs [52/128] train_loss: 0.95054 val_loss: 0.10211\n",
      "epochs [53/128] train_loss: 0.90566 val_loss: 0.17597\n",
      "epochs [54/128] train_loss: 1.17814 val_loss: 0.09721\n",
      "epochs [55/128] train_loss: 0.92286 val_loss: 0.11023\n",
      "epochs [56/128] train_loss: 0.92220 val_loss: 0.12552\n",
      "epochs [57/128] train_loss: 0.95531 val_loss: 0.12039\n",
      "epochs [58/128] train_loss: 0.86153 val_loss: 0.09902\n",
      "epochs [59/128] train_loss: 0.89537 val_loss: 0.11898\n",
      "epochs [60/128] train_loss: 0.93151 val_loss: 0.08929\n",
      "epochs [61/128] train_loss: 0.89622 val_loss: 0.10437\n",
      "epochs [62/128] train_loss: 0.81971 val_loss: 0.08919\n",
      "epochs [63/128] train_loss: 0.80502 val_loss: 0.10283\n",
      "epochs [64/128] train_loss: 0.82867 val_loss: 0.12023\n",
      "epochs [65/128] train_loss: 0.90532 val_loss: 0.09117\n",
      "epochs [66/128] train_loss: 0.83389 val_loss: 0.10456\n",
      "epochs [67/128] train_loss: 0.90525 val_loss: 0.09993\n",
      "epochs [68/128] train_loss: 0.84720 val_loss: 0.11607\n",
      "epochs [69/128] train_loss: 0.80608 val_loss: 0.08744\n",
      "epochs [70/128] train_loss: 0.81677 val_loss: 0.11425\n",
      "epochs [71/128] train_loss: 0.84008 val_loss: 0.13166\n",
      "epochs [72/128] train_loss: 0.83257 val_loss: 0.08337\n",
      "epochs [73/128] train_loss: 0.79272 val_loss: 0.12528\n",
      "epochs [74/128] train_loss: 0.78794 val_loss: 0.10931\n",
      "epochs [75/128] train_loss: 0.79397 val_loss: 0.08178\n",
      "epochs [76/128] train_loss: 0.73817 val_loss: 0.12963\n",
      "epochs [77/128] train_loss: 0.84308 val_loss: 0.08615\n",
      "epochs [78/128] train_loss: 0.76008 val_loss: 0.08891\n",
      "epochs [79/128] train_loss: 0.75966 val_loss: 0.09266\n",
      "epochs [80/128] train_loss: 0.75559 val_loss: 0.08163\n",
      "epochs [81/128] train_loss: 0.73371 val_loss: 0.08661\n",
      "epochs [82/128] train_loss: 0.74746 val_loss: 0.07960\n",
      "epochs [83/128] train_loss: 0.71289 val_loss: 0.07754\n",
      "epochs [84/128] train_loss: 0.69048 val_loss: 0.10388\n",
      "epochs [85/128] train_loss: 0.76797 val_loss: 0.09857\n",
      "epochs [86/128] train_loss: 0.74894 val_loss: 0.09412\n",
      "epochs [87/128] train_loss: 0.77495 val_loss: 0.07601\n",
      "epochs [88/128] train_loss: 0.70457 val_loss: 0.08432\n",
      "epochs [89/128] train_loss: 0.66557 val_loss: 0.07476\n",
      "epochs [90/128] train_loss: 0.70256 val_loss: 0.08024\n",
      "epochs [91/128] train_loss: 0.72685 val_loss: 0.12856\n",
      "epochs [92/128] train_loss: 0.73450 val_loss: 0.07247\n",
      "epochs [93/128] train_loss: 0.69799 val_loss: 0.11117\n",
      "epochs [94/128] train_loss: 0.71123 val_loss: 0.07468\n",
      "epochs [95/128] train_loss: 0.68182 val_loss: 0.08129\n",
      "epochs [96/128] train_loss: 0.71717 val_loss: 0.14193\n",
      "epochs [97/128] train_loss: 0.79201 val_loss: 0.08912\n",
      "epochs [98/128] train_loss: 0.77576 val_loss: 0.13599\n",
      "epochs [99/128] train_loss: 0.80458 val_loss: 0.08213\n",
      "epochs [100/128] train_loss: 0.68084 val_loss: 0.07973\n",
      "epochs [101/128] train_loss: 0.71476 val_loss: 0.08320\n",
      "epochs [102/128] train_loss: 0.75057 val_loss: 0.09674\n",
      "epochs [103/128] train_loss: 0.67505 val_loss: 0.07952\n",
      "epochs [104/128] train_loss: 0.65257 val_loss: 0.07118\n",
      "epochs [105/128] train_loss: 0.67696 val_loss: 0.07215\n",
      "epochs [106/128] train_loss: 0.63773 val_loss: 0.08776\n",
      "epochs [107/128] train_loss: 0.75112 val_loss: 0.08793\n",
      "epochs [108/128] train_loss: 0.62926 val_loss: 0.07762\n",
      "epochs [109/128] train_loss: 0.61885 val_loss: 0.06483\n",
      "epochs [110/128] train_loss: 0.66587 val_loss: 0.06717\n",
      "epochs [111/128] train_loss: 0.59486 val_loss: 0.07890\n",
      "epochs [112/128] train_loss: 0.62095 val_loss: 0.10294\n",
      "epochs [113/128] train_loss: 0.69957 val_loss: 0.07528\n",
      "epochs [114/128] train_loss: 0.62938 val_loss: 0.07075\n",
      "epochs [115/128] train_loss: 0.60058 val_loss: 0.07136\n",
      "epochs [116/128] train_loss: 0.63920 val_loss: 0.06903\n",
      "epochs [117/128] train_loss: 0.63355 val_loss: 0.06374\n",
      "epochs [118/128] train_loss: 0.60638 val_loss: 0.06204\n",
      "epochs [119/128] train_loss: 0.61025 val_loss: 0.07249\n",
      "epochs [120/128] train_loss: 0.62937 val_loss: 0.09184\n",
      "epochs [121/128] train_loss: 0.66799 val_loss: 0.07011\n",
      "epochs [122/128] train_loss: 0.60096 val_loss: 0.06924\n",
      "epochs [123/128] train_loss: 0.62268 val_loss: 0.12232\n",
      "epochs [124/128] train_loss: 0.65397 val_loss: 0.07206\n",
      "epochs [125/128] train_loss: 0.63408 val_loss: 0.06347\n",
      "epochs [126/128] train_loss: 0.59261 val_loss: 0.06685\n",
      "epochs [127/128] train_loss: 0.61358 val_loss: 0.06840\n",
      "epochs [128/128] train_loss: 0.63590 val_loss: 0.10470\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)\n",
    "    train_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    val_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    # training\n",
    "    total_train_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(train_dataset):\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "            X_wide_train_cuda = batch_x[0].float().cuda()\n",
    "            X_deep_train_cuda = batch_x[1].float().cuda()\n",
    "            y_train_cuda = batch_y.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        pred_y = net(X_wide_train_cuda, X_deep_train_cuda)\n",
    "        loss = criterion(pred_y, y_train_cuda)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # validating\n",
    "    total_val_loss = 0\n",
    "    for _,(batch_val_x, batch_val_y) in enumerate(val_dataset):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_val_cuda = batch_val_x[0].float().cuda()\n",
    "            X_deep_val_cuda = batch_val_x[1].float().cuda()\n",
    "            y_val_cuda = batch_val_y.cuda()\n",
    "        \n",
    "        pred_y = net(X_wide_val_cuda, X_deep_val_cuda)\n",
    "        val_loss = criterion(pred_y, y_val_cuda)\n",
    "        total_val_loss += val_loss.item()\n",
    "        # print statistics\n",
    "    if min_val_loss > total_val_loss:\n",
    "        torch.save(net.state_dict(), model_name)\n",
    "        min_val_loss = total_val_loss\n",
    "    print('epochs [%d/%d] train_loss: %.5f val_loss: %.5f' % (epoch + 1, epochs, total_train_loss, total_val_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:51:28.383932Z",
     "start_time": "2021-08-03T17:51:27.989004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('./model_saver/'+model_name))\n",
    "\n",
    "years = test[4].unique()\n",
    "\n",
    "test_list = []\n",
    "\n",
    "for year in years:\n",
    "    temp = test[test[4]==year]\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    test_list.append(temp)\n",
    "    \n",
    "len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T17:51:35.225220Z",
     "start_time": "2021-08-03T17:51:28.386326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 年:\n",
      "avg wind error: [4.38470532]\n",
      "2016 年:\n",
      "avg wind error: [4.8975171]\n",
      "2017 年:\n",
      "avg wind error: [3.90941605]\n",
      "2018 年:\n",
      "avg wind error: [4.01521519]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for year, _test in zip(years, test_list):\n",
    "\n",
    "        print(year, '年:')\n",
    "        y_test = _test.loc[:,3]\n",
    "        X_wide_test = X_wide_scaler.transform(_test.loc[:,5:])\n",
    "\n",
    "        final_test_u_list = []\n",
    "        for ahead_time in ahead_times:\n",
    "            year_test_list = []\n",
    "            for pressure in pressures:\n",
    "                scaler_name = 'u' +str(pressure) + str(ahead_time)\n",
    "                X_deep = reanalysis_u_test_dict[scaler_name][reanalysis_u_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
    "                X_deep = X_deep_u_scaler_dict[scaler_name].transform(X_deep)\n",
    "                X_deep_final = X_deep.reshape(-1, 1, 1, 31, 31, 1)\n",
    "                year_test_list.append(X_deep_final)\n",
    "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
    "            final_test_u_list.append(X_deep_temp)\n",
    "        X_deep_u_test = np.concatenate(final_test_u_list, axis=5)\n",
    "        \n",
    "        final_test_v_list = []\n",
    "        for ahead_time in ahead_times:\n",
    "            year_test_list = []\n",
    "            for pressure in pressures:\n",
    "                scaler_name = 'v' +str(pressure) + str(ahead_time)\n",
    "                X_deep = reanalysis_v_test_dict[scaler_name][reanalysis_v_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
    "                X_deep = X_deep_v_scaler_dict[scaler_name].transform(X_deep)\n",
    "                X_deep_final = X_deep.reshape(-1, 1, 1, 31, 31, 1)\n",
    "                year_test_list.append(X_deep_final)\n",
    "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
    "            final_test_v_list.append(X_deep_temp)\n",
    "        X_deep_v_test = np.concatenate(final_test_v_list, axis=5)\n",
    "    \n",
    "        X_deep_test = np.concatenate((X_deep_u_test, X_deep_v_test), axis=1)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_test = Variable(torch.from_numpy(X_wide_test).float().cuda())\n",
    "            X_deep_test = Variable(torch.from_numpy(X_deep_test).float().cuda())\n",
    "\n",
    "        pred = net(X_wide_test, X_deep_test)\n",
    "\n",
    "        pred = y_scaler.inverse_transform(pred.cpu().detach().numpy().reshape(-1,1))\n",
    "        true = y_test.values.reshape(-1, 1)\n",
    "        diff = np.abs(pred - true)\n",
    "\n",
    "        print('avg wind error:', sum(diff)/len(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
