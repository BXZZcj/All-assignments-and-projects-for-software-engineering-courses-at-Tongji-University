{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:04.264804Z",
     "start_time": "2021-08-04T03:33:03.391385Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:05.851269Z",
     "start_time": "2021-08-04T03:33:05.846320Z"
    }
   },
   "outputs": [],
   "source": [
    "#  predition TI of leading time at 24 hours\n",
    "pre_seq = 4\n",
    "batch_size = 128\n",
    "epochs = 128\n",
    "min_val_loss = 100\n",
    "model_name = '3D_SAF_Net(4).pkl'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:06.410580Z",
     "start_time": "2021-08-04T03:33:06.255580Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/CMA_train_'+str(pre_seq*6)+'h.csv', header=None)\n",
    "test= pd.read_csv('../../data/CMA_test_'+str(pre_seq*6)+'h.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:06.671255Z",
     "start_time": "2021-08-04T03:33:06.661935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8406, 101), (2747, 101))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:07.081786Z",
     "start_time": "2021-08-04T03:33:07.053997Z"
    }
   },
   "outputs": [],
   "source": [
    "CLIPER_feature =  pd.concat((train, test), axis=0)\n",
    "CLIPER_feature.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:07.470238Z",
     "start_time": "2021-08-04T03:33:07.414209Z"
    }
   },
   "outputs": [],
   "source": [
    "X_wide_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X_wide = X_wide_scaler.fit_transform(CLIPER_feature.iloc[:, 5:])\n",
    "X_wide_train = X_wide[0: train.shape[0], :]\n",
    "\n",
    "y = y_scaler.fit_transform(CLIPER_feature.loc[:, 3].values.reshape(-1,1))\n",
    "y_train = y[0: train.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:07.779145Z",
     "start_time": "2021-08-04T03:33:07.774012Z"
    }
   },
   "outputs": [],
   "source": [
    "# now 6 hours ago  12 hours ago  18 hour ago\n",
    "ahead_times = [0,1,2,3]\n",
    "\n",
    "pressures = [1000, 750, 500, 250]\n",
    "\n",
    "sequential_reanalysis_u_list = []\n",
    "reanalysis_u_test_dict = {}\n",
    "X_deep_u_scaler_dict = {}\n",
    "\n",
    "sequential_reanalysis_v_list = []\n",
    "reanalysis_v_test_dict = {}\n",
    "X_deep_v_scaler_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:29.901051Z",
     "start_time": "2021-08-04T03:33:08.606836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahead_time: 0 (8406, 1, 4, 31, 31, 1)\n",
      "ahead_time: 1 (8406, 1, 4, 31, 31, 1)\n",
      "ahead_time: 2 (8406, 1, 4, 31, 31, 1)\n",
      "ahead_time: 3 (8406, 1, 4, 31, 31, 1)\n"
     ]
    }
   ],
   "source": [
    "reanalysis_type = 'u'\n",
    "for ahead_time in ahead_times:\n",
    "\n",
    "    reanalysis_list = []\n",
    "    for pressure in pressures:\n",
    "        folder = None\n",
    "        if ahead_time == 0:\n",
    "            folder = reanalysis_type\n",
    "        else:\n",
    "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
    "            \n",
    "        train_reanalysis_csv = pd.read_csv('../../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_train_31_31.csv', header=None)\n",
    "        test_reanalysis_csv = pd.read_csv('../../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_test_31_31.csv', header=None)\n",
    "\n",
    "        train_reanalysis = train_reanalysis_csv[train_reanalysis_csv[0].isin(train[0].unique())]\n",
    "        test_reanalysis = test_reanalysis_csv[test_reanalysis_csv[0].isin(test[0].unique())]\n",
    "        reanalysis_u_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
    "        \n",
    "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
    "        reanalysis.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "        X_deep_u_scaler_dict[scaler_name] = MinMaxScaler()\n",
    "        \n",
    "        # 5:end is the 31*31 u component wind speed\n",
    "        X_deep = X_deep_u_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
    "        \n",
    "         # (batch, type, channel, height, widht, time) here type is u\n",
    "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 31, 31, 1)\n",
    "        reanalysis_list.append(X_deep_final)\n",
    "        \n",
    "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
    "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
    "    sequential_reanalysis_u_list.append(X_deep_temp)\n",
    "\n",
    "X_deep_u_train = np.concatenate(sequential_reanalysis_u_list, axis=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:50.849063Z",
     "start_time": "2021-08-04T03:33:29.902827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahead_time: 0 (8406, 1, 4, 31, 31, 1)\n",
      "ahead_time: 1 (8406, 1, 4, 31, 31, 1)\n",
      "ahead_time: 2 (8406, 1, 4, 31, 31, 1)\n",
      "ahead_time: 3 (8406, 1, 4, 31, 31, 1)\n"
     ]
    }
   ],
   "source": [
    "reanalysis_type = 'v'\n",
    "for ahead_time in ahead_times:\n",
    "\n",
    "    reanalysis_list = []\n",
    "    for pressure in pressures:\n",
    "        folder = None\n",
    "        if ahead_time == 0:\n",
    "            folder = reanalysis_type\n",
    "        else:\n",
    "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
    "\n",
    "        train_reanalysis_csv = pd.read_csv('../../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_train_31_31.csv', header=None)\n",
    "        test_reanalysis_csv = pd.read_csv('../../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_test_31_31.csv', header=None)\n",
    "\n",
    "        train_reanalysis = train_reanalysis_csv[train_reanalysis_csv[0].isin(train[0].unique())]\n",
    "        test_reanalysis = test_reanalysis_csv[test_reanalysis_csv[0].isin(test[0].unique())]\n",
    "        reanalysis_v_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
    "\n",
    "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
    "        reanalysis.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "        X_deep_v_scaler_dict[scaler_name] = MinMaxScaler()\n",
    "        \n",
    "        # 5:end is the 31*31 u component wind speed\n",
    "        X_deep = X_deep_v_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
    "        # (batch, type, channel, height, widht, time) here type is u\n",
    "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 31, 31, 1)\n",
    "        reanalysis_list.append(X_deep_final)\n",
    "        \n",
    "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
    "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
    "    sequential_reanalysis_v_list.append(X_deep_temp)\n",
    "\n",
    "X_deep_v_train = np.concatenate(sequential_reanalysis_v_list, axis=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:51.468765Z",
     "start_time": "2021-08-04T03:33:50.850807Z"
    }
   },
   "outputs": [],
   "source": [
    "X_deep_train = np.concatenate((X_deep_u_train, X_deep_v_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:51.473634Z",
     "start_time": "2021-08-04T03:33:51.470604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8406, 96), (8406, 2, 4, 31, 31, 4))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_train.shape, X_deep_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:51.525627Z",
     "start_time": "2021-08-04T03:33:51.474619Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainLoader(Data.Dataset):\n",
    "    def __init__(self, X_wide_train, X_deep_train, y_train):\n",
    "        self.X_wide_train = X_wide_train\n",
    "        self.X_deep_train = X_deep_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return [self.X_wide_train[index], self.X_deep_train[index]], self.y_train[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_wide_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:51.611575Z",
     "start_time": "2021-08-04T03:33:51.527884Z"
    }
   },
   "outputs": [],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:51.693948Z",
     "start_time": "2021-08-04T03:33:51.613996Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7565, 841)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_index), len(val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:52.498674Z",
     "start_time": "2021-08-04T03:33:51.697377Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:52.581585Z",
     "start_time": "2021-08-04T03:33:52.500057Z"
    }
   },
   "outputs": [],
   "source": [
    "val_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D SAF-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:52.626073Z",
     "start_time": "2021-08-04T03:33:52.583693Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(2, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        self.fc1 = nn.Linear(64 * 15 * 15 * 2, 128)\n",
    "        self.fc2 = nn.Linear(96 + 128*4, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, wide, deep):\n",
    "        seq_list = []\n",
    "        for i in range(len(ahead_times)):\n",
    "            timeseq = deep[...,i]\n",
    "            timeseq = self.pool1(F.relu(self.conv1(timeseq)))\n",
    "            timeseq = timeseq.view(-1, 64 * 15 * 15 * 2)\n",
    "            timeseq = self.fc1(timeseq)\n",
    "            seq_list.append(timeseq)\n",
    "        wide = wide.view(-1, 96)\n",
    "        wide_n_deep = torch.cat((wide, seq_list[0]),1)\n",
    "        if len(ahead_times) > 1:\n",
    "            for i in range(1, len(ahead_times)):\n",
    "                wide_n_deep = torch.cat((wide_n_deep, seq_list[i]),1)\n",
    "        wide_n_deep = self.fc2(wide_n_deep)\n",
    "        wide_n_deep = self.fc3(wide_n_deep)\n",
    "        wide_n_deep = self.fc4(wide_n_deep)\n",
    "        return wide_n_deep\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:53.985792Z",
     "start_time": "2021-08-04T03:33:52.627038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv3d(2, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool1): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=28800, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=608, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = net.to(device)\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:53.989118Z",
     "start_time": "2021-08-04T03:33:53.986851Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T09:39:13.062661Z",
     "start_time": "2021-07-30T09:33:20.811454Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs [1/128] cost:3.00s train_loss: 25.72493 val_loss: 0.70072\n",
      "epochs [2/128] cost:2.00s train_loss: 5.12769 val_loss: 0.57045\n",
      "epochs [3/128] cost:2.00s train_loss: 4.33960 val_loss: 0.47098\n",
      "epochs [4/128] cost:2.00s train_loss: 4.09992 val_loss: 0.60251\n",
      "epochs [5/128] cost:2.00s train_loss: 4.04944 val_loss: 0.50076\n",
      "epochs [6/128] cost:2.00s train_loss: 3.84244 val_loss: 0.41939\n",
      "epochs [7/128] cost:2.00s train_loss: 3.77819 val_loss: 0.43701\n",
      "epochs [8/128] cost:2.00s train_loss: 3.75003 val_loss: 0.44400\n",
      "epochs [9/128] cost:2.00s train_loss: 3.50568 val_loss: 0.45114\n",
      "epochs [10/128] cost:2.00s train_loss: 3.44077 val_loss: 0.43521\n",
      "epochs [11/128] cost:2.00s train_loss: 3.30729 val_loss: 0.36046\n",
      "epochs [12/128] cost:2.00s train_loss: 3.17345 val_loss: 0.34409\n",
      "epochs [13/128] cost:2.00s train_loss: 3.23998 val_loss: 0.36457\n",
      "epochs [14/128] cost:2.00s train_loss: 3.10618 val_loss: 0.40266\n",
      "epochs [15/128] cost:2.00s train_loss: 2.98502 val_loss: 0.34394\n",
      "epochs [16/128] cost:2.00s train_loss: 2.82378 val_loss: 0.32690\n",
      "epochs [17/128] cost:2.00s train_loss: 2.95025 val_loss: 0.34712\n",
      "epochs [18/128] cost:2.00s train_loss: 2.85405 val_loss: 0.39404\n",
      "epochs [19/128] cost:3.00s train_loss: 2.72510 val_loss: 0.32093\n",
      "epochs [20/128] cost:2.00s train_loss: 2.60411 val_loss: 0.30166\n",
      "epochs [21/128] cost:2.00s train_loss: 2.58422 val_loss: 0.35194\n",
      "epochs [22/128] cost:2.00s train_loss: 2.54551 val_loss: 0.29363\n",
      "epochs [23/128] cost:2.00s train_loss: 2.44356 val_loss: 0.26250\n",
      "epochs [24/128] cost:2.00s train_loss: 2.39350 val_loss: 0.47095\n",
      "epochs [25/128] cost:2.00s train_loss: 2.48453 val_loss: 0.33203\n",
      "epochs [26/128] cost:2.00s train_loss: 2.37203 val_loss: 0.26415\n",
      "epochs [27/128] cost:2.00s train_loss: 2.34980 val_loss: 0.29868\n",
      "epochs [28/128] cost:2.00s train_loss: 2.43342 val_loss: 0.26484\n",
      "epochs [29/128] cost:2.00s train_loss: 2.39414 val_loss: 0.30076\n",
      "epochs [30/128] cost:2.00s train_loss: 2.35021 val_loss: 0.29145\n",
      "epochs [31/128] cost:2.00s train_loss: 2.24268 val_loss: 0.24081\n",
      "epochs [32/128] cost:2.00s train_loss: 2.21100 val_loss: 0.32722\n",
      "epochs [33/128] cost:2.00s train_loss: 2.22321 val_loss: 0.25058\n",
      "epochs [34/128] cost:2.00s train_loss: 2.09298 val_loss: 0.23370\n",
      "epochs [35/128] cost:2.00s train_loss: 2.13418 val_loss: 0.23108\n",
      "epochs [36/128] cost:2.00s train_loss: 2.02979 val_loss: 0.23065\n",
      "epochs [37/128] cost:2.00s train_loss: 1.97337 val_loss: 0.25704\n",
      "epochs [38/128] cost:2.00s train_loss: 2.00743 val_loss: 0.21160\n",
      "epochs [39/128] cost:2.00s train_loss: 2.08460 val_loss: 0.29703\n",
      "epochs [40/128] cost:2.00s train_loss: 2.04487 val_loss: 0.21296\n",
      "epochs [41/128] cost:2.00s train_loss: 1.83401 val_loss: 0.21688\n",
      "epochs [42/128] cost:2.00s train_loss: 1.85275 val_loss: 0.24868\n",
      "epochs [43/128] cost:2.00s train_loss: 1.88071 val_loss: 0.22600\n",
      "epochs [44/128] cost:2.00s train_loss: 1.83646 val_loss: 0.21494\n",
      "epochs [45/128] cost:2.00s train_loss: 2.01632 val_loss: 0.24981\n",
      "epochs [46/128] cost:2.00s train_loss: 1.87613 val_loss: 0.21229\n",
      "epochs [47/128] cost:2.00s train_loss: 1.95954 val_loss: 0.20606\n",
      "epochs [48/128] cost:2.00s train_loss: 1.75464 val_loss: 0.19249\n",
      "epochs [49/128] cost:2.00s train_loss: 1.81390 val_loss: 0.20022\n",
      "epochs [50/128] cost:2.00s train_loss: 1.83424 val_loss: 0.28193\n",
      "epochs [51/128] cost:2.00s train_loss: 1.83078 val_loss: 0.30067\n",
      "epochs [52/128] cost:2.00s train_loss: 1.85207 val_loss: 0.27119\n",
      "epochs [53/128] cost:2.00s train_loss: 1.75367 val_loss: 0.20530\n",
      "epochs [54/128] cost:2.00s train_loss: 1.75056 val_loss: 0.18897\n",
      "epochs [55/128] cost:2.00s train_loss: 1.62849 val_loss: 0.19867\n",
      "epochs [56/128] cost:2.00s train_loss: 1.68186 val_loss: 0.25646\n",
      "epochs [57/128] cost:2.00s train_loss: 1.69014 val_loss: 0.25957\n",
      "epochs [58/128] cost:2.00s train_loss: 1.65533 val_loss: 0.20448\n",
      "epochs [59/128] cost:2.00s train_loss: 1.62656 val_loss: 0.17887\n",
      "epochs [60/128] cost:2.00s train_loss: 1.65144 val_loss: 0.24982\n",
      "epochs [61/128] cost:2.00s train_loss: 1.57796 val_loss: 0.22854\n",
      "epochs [62/128] cost:2.00s train_loss: 1.72930 val_loss: 0.19513\n",
      "epochs [63/128] cost:2.00s train_loss: 1.60370 val_loss: 0.21829\n",
      "epochs [64/128] cost:2.00s train_loss: 1.59328 val_loss: 0.16813\n",
      "epochs [65/128] cost:2.00s train_loss: 1.58320 val_loss: 0.28283\n",
      "epochs [66/128] cost:2.00s train_loss: 1.81197 val_loss: 0.18443\n",
      "epochs [67/128] cost:2.00s train_loss: 1.52762 val_loss: 0.17741\n",
      "epochs [68/128] cost:2.00s train_loss: 1.60046 val_loss: 0.16564\n",
      "epochs [69/128] cost:2.00s train_loss: 1.52214 val_loss: 0.17222\n",
      "epochs [70/128] cost:2.00s train_loss: 1.52074 val_loss: 0.18792\n",
      "epochs [71/128] cost:2.00s train_loss: 1.66603 val_loss: 0.18241\n",
      "epochs [72/128] cost:2.00s train_loss: 1.57218 val_loss: 0.18990\n",
      "epochs [73/128] cost:2.00s train_loss: 1.62285 val_loss: 0.18509\n",
      "epochs [74/128] cost:2.00s train_loss: 1.48075 val_loss: 0.25565\n",
      "epochs [75/128] cost:2.00s train_loss: 1.47045 val_loss: 0.15800\n",
      "epochs [76/128] cost:2.00s train_loss: 1.37255 val_loss: 0.17286\n",
      "epochs [77/128] cost:2.00s train_loss: 1.48876 val_loss: 0.18203\n",
      "epochs [78/128] cost:2.00s train_loss: 1.39978 val_loss: 0.15431\n",
      "epochs [79/128] cost:2.00s train_loss: 1.40346 val_loss: 0.27432\n",
      "epochs [80/128] cost:2.00s train_loss: 1.50380 val_loss: 0.16441\n",
      "epochs [81/128] cost:2.00s train_loss: 1.38034 val_loss: 0.16796\n",
      "epochs [82/128] cost:2.00s train_loss: 1.41850 val_loss: 0.17317\n",
      "epochs [83/128] cost:2.00s train_loss: 1.32591 val_loss: 0.22989\n",
      "epochs [84/128] cost:2.00s train_loss: 1.44889 val_loss: 0.16350\n",
      "epochs [85/128] cost:2.00s train_loss: 1.42149 val_loss: 0.16177\n",
      "epochs [86/128] cost:2.00s train_loss: 1.27448 val_loss: 0.18034\n",
      "epochs [87/128] cost:2.00s train_loss: 1.35665 val_loss: 0.17873\n",
      "epochs [88/128] cost:2.00s train_loss: 1.31598 val_loss: 0.21674\n",
      "epochs [89/128] cost:2.00s train_loss: 1.35736 val_loss: 0.14395\n",
      "epochs [90/128] cost:2.00s train_loss: 1.34771 val_loss: 0.15085\n",
      "epochs [91/128] cost:2.00s train_loss: 1.29877 val_loss: 0.16453\n",
      "epochs [92/128] cost:2.00s train_loss: 1.24923 val_loss: 0.32674\n",
      "epochs [93/128] cost:2.00s train_loss: 1.49520 val_loss: 0.15852\n",
      "epochs [94/128] cost:2.00s train_loss: 1.28168 val_loss: 0.14123\n",
      "epochs [95/128] cost:2.00s train_loss: 1.23942 val_loss: 0.13900\n",
      "epochs [96/128] cost:2.00s train_loss: 1.37819 val_loss: 0.29658\n",
      "epochs [97/128] cost:2.00s train_loss: 1.48087 val_loss: 0.15587\n",
      "epochs [98/128] cost:2.00s train_loss: 1.22996 val_loss: 0.18101\n",
      "epochs [99/128] cost:2.00s train_loss: 1.30081 val_loss: 0.18874\n",
      "epochs [100/128] cost:2.00s train_loss: 1.30637 val_loss: 0.14294\n",
      "epochs [101/128] cost:2.00s train_loss: 1.20437 val_loss: 0.17291\n",
      "epochs [102/128] cost:2.00s train_loss: 1.25016 val_loss: 0.15287\n",
      "epochs [103/128] cost:2.00s train_loss: 1.29694 val_loss: 0.23883\n",
      "epochs [104/128] cost:2.00s train_loss: 1.25736 val_loss: 0.17221\n",
      "epochs [105/128] cost:2.00s train_loss: 1.30954 val_loss: 0.12477\n",
      "epochs [106/128] cost:2.00s train_loss: 1.19854 val_loss: 0.14530\n",
      "epochs [107/128] cost:2.00s train_loss: 1.26624 val_loss: 0.13809\n",
      "epochs [108/128] cost:2.00s train_loss: 1.27287 val_loss: 0.14513\n",
      "epochs [109/128] cost:2.00s train_loss: 1.28826 val_loss: 0.13377\n",
      "epochs [110/128] cost:2.00s train_loss: 1.23527 val_loss: 0.17791\n",
      "epochs [111/128] cost:2.00s train_loss: 1.18242 val_loss: 0.12942\n",
      "epochs [112/128] cost:2.00s train_loss: 1.15991 val_loss: 0.17166\n",
      "epochs [113/128] cost:2.00s train_loss: 1.23756 val_loss: 0.15063\n",
      "epochs [114/128] cost:2.00s train_loss: 1.17958 val_loss: 0.17206\n",
      "epochs [115/128] cost:2.00s train_loss: 1.34932 val_loss: 0.13080\n",
      "epochs [116/128] cost:2.00s train_loss: 1.27008 val_loss: 0.13054\n",
      "epochs [117/128] cost:2.00s train_loss: 1.18947 val_loss: 0.17409\n",
      "epochs [118/128] cost:2.00s train_loss: 1.14883 val_loss: 0.14518\n",
      "epochs [119/128] cost:2.00s train_loss: 1.10748 val_loss: 0.12214\n",
      "epochs [120/128] cost:2.00s train_loss: 1.09699 val_loss: 0.13745\n",
      "epochs [121/128] cost:2.00s train_loss: 1.06355 val_loss: 0.11607\n",
      "epochs [122/128] cost:2.00s train_loss: 1.06406 val_loss: 0.16664\n",
      "epochs [123/128] cost:2.00s train_loss: 1.11938 val_loss: 0.12929\n",
      "epochs [124/128] cost:2.00s train_loss: 1.05564 val_loss: 0.14576\n",
      "epochs [125/128] cost:2.00s train_loss: 1.07295 val_loss: 0.12023\n",
      "epochs [126/128] cost:2.00s train_loss: 1.22265 val_loss: 0.13031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs [127/128] cost:2.00s train_loss: 1.17572 val_loss: 0.12817\n",
      "epochs [128/128] cost:2.00s train_loss: 1.10485 val_loss: 0.15934\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    starttime = datetime.datetime.now()\n",
    "    train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)\n",
    "    train_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    val_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    total_train_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(train_dataset):\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "            X_wide_train_cuda = batch_x[0].float().cuda()\n",
    "            X_deep_train_cuda = batch_x[1].float().cuda()\n",
    "            y_train_cuda = batch_y.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        pred_y = net(X_wide_train_cuda, X_deep_train_cuda)\n",
    "        loss = criterion(pred_y, y_train_cuda)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    total_val_loss = 0\n",
    "    for _,(batch_val_x, batch_val_y) in enumerate(val_dataset):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_val_cuda = batch_val_x[0].float().cuda()\n",
    "            X_deep_val_cuda = batch_val_x[1].float().cuda()\n",
    "            y_val_cuda = batch_val_y.cuda()\n",
    "        \n",
    "        pred_y = net(X_wide_val_cuda, X_deep_val_cuda)\n",
    "        val_loss = criterion(pred_y, y_val_cuda)\n",
    "        total_val_loss += val_loss.item()\n",
    "    \n",
    "        # print statistics\n",
    "    if min_val_loss > total_val_loss:\n",
    "        torch.save(net.state_dict(), model_name)\n",
    "        min_val_loss = total_val_loss\n",
    "    endtime = datetime.datetime.now()\n",
    "    print('epochs [%d/%d] cost:%.2fs train_loss: %.5f val_loss: %.5f' % \n",
    "          (epoch + 1, epochs, (endtime-starttime).seconds, total_train_loss, total_val_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:54.084941Z",
     "start_time": "2021-08-04T03:33:53.990031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(model_name))\n",
    "\n",
    "years = test[4].unique()\n",
    "\n",
    "test_list = []\n",
    "\n",
    "for year in years:\n",
    "    temp = test[test[4]==year]\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    test_list.append(temp)\n",
    "    \n",
    "len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T03:33:56.661560Z",
     "start_time": "2021-08-04T03:33:54.086596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 年:\n",
      "avg wind error: [6.13563475]\n",
      "2016 年:\n",
      "avg wind error: [6.16722333]\n",
      "2017 年:\n",
      "avg wind error: [5.58428035]\n",
      "2018 年:\n",
      "avg wind error: [6.27989368]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for year, _test in zip(years, test_list):\n",
    "\n",
    "        print(year, '年:')\n",
    "        y_test = _test.loc[:,3]\n",
    "        X_wide_test = X_wide_scaler.transform(_test.loc[:,5:])\n",
    "\n",
    "        final_test_u_list = []\n",
    "        for ahead_time in ahead_times:\n",
    "            year_test_list = []\n",
    "            for pressure in pressures:\n",
    "                scaler_name = 'u' +str(pressure) + str(ahead_time)\n",
    "                X_deep = reanalysis_u_test_dict[scaler_name][reanalysis_u_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
    "                X_deep = X_deep_u_scaler_dict[scaler_name].transform(X_deep)\n",
    "                X_deep_final = X_deep.reshape(-1, 1, 1, 31, 31, 1)\n",
    "                year_test_list.append(X_deep_final)\n",
    "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
    "            final_test_u_list.append(X_deep_temp)\n",
    "        X_deep_u_test = np.concatenate(final_test_u_list, axis=5)\n",
    "        \n",
    "        final_test_v_list = []\n",
    "        for ahead_time in ahead_times:\n",
    "            year_test_list = []\n",
    "            for pressure in pressures:\n",
    "                scaler_name = 'v' +str(pressure) + str(ahead_time)\n",
    "                X_deep = reanalysis_v_test_dict[scaler_name][reanalysis_v_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
    "                X_deep = X_deep_v_scaler_dict[scaler_name].transform(X_deep)\n",
    "                X_deep_final = X_deep.reshape(-1, 1, 1, 31, 31, 1)\n",
    "                year_test_list.append(X_deep_final)\n",
    "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
    "            final_test_v_list.append(X_deep_temp)\n",
    "        X_deep_v_test = np.concatenate(final_test_v_list, axis=5)\n",
    "    \n",
    "        X_deep_test = np.concatenate((X_deep_u_test, X_deep_v_test), axis=1)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_test = Variable(torch.from_numpy(X_wide_test).float().cuda())\n",
    "            X_deep_test = Variable(torch.from_numpy(X_deep_test).float().cuda())\n",
    "\n",
    "        pred = net(X_wide_test, X_deep_test)\n",
    "\n",
    "        pred = y_scaler.inverse_transform(pred.cpu().detach().numpy().reshape(-1,1))\n",
    "        true = y_test.values.reshape(-1, 1)\n",
    "        diff = np.abs(pred - true)\n",
    "\n",
    "        print('avg wind error:', sum(diff)/len(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
