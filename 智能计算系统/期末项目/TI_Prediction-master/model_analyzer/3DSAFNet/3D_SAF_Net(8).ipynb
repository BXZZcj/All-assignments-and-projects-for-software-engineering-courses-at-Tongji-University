{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:16:37.468247Z",
     "start_time": "2021-08-04T04:16:36.591692Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:16:37.471978Z",
     "start_time": "2021-08-04T04:16:37.469722Z"
    }
   },
   "outputs": [],
   "source": [
    "#  predition TI of leading time at 24 hours\n",
    "pre_seq = 4\n",
    "batch_size = 128\n",
    "epochs = 128\n",
    "min_val_loss = 100\n",
    "model_name = '3D_SAF_Net(8).pkl'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:16:37.678922Z",
     "start_time": "2021-08-04T04:16:37.480309Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/CMA_train_'+str(pre_seq*6)+'h.csv', header=None)\n",
    "test= pd.read_csv('../../data/CMA_test_'+str(pre_seq*6)+'h.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:16:37.765948Z",
     "start_time": "2021-08-04T04:16:37.757272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8406, 101), (2747, 101))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:16:38.105498Z",
     "start_time": "2021-08-04T04:16:38.078334Z"
    }
   },
   "outputs": [],
   "source": [
    "CLIPER_feature =  pd.concat((train, test), axis=0)\n",
    "CLIPER_feature.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:16:38.505687Z",
     "start_time": "2021-08-04T04:16:38.447492Z"
    }
   },
   "outputs": [],
   "source": [
    "X_wide_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X_wide = X_wide_scaler.fit_transform(CLIPER_feature.iloc[:, 5:])\n",
    "X_wide_train = X_wide[0: train.shape[0], :]\n",
    "\n",
    "y = y_scaler.fit_transform(CLIPER_feature.loc[:, 3].values.reshape(-1,1))\n",
    "y_train = y[0: train.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:16:38.835005Z",
     "start_time": "2021-08-04T04:16:38.829269Z"
    }
   },
   "outputs": [],
   "source": [
    "# now 6 hours ago  12 hours ago  18 hour ago\n",
    "ahead_times = [0,1,2,3]\n",
    "\n",
    "pressures = [1000, 900, 800, 700, 600, 500, 400, 300]\n",
    "\n",
    "sequential_reanalysis_u_list = []\n",
    "reanalysis_u_test_dict = {}\n",
    "X_deep_u_scaler_dict = {}\n",
    "\n",
    "sequential_reanalysis_v_list = []\n",
    "reanalysis_v_test_dict = {}\n",
    "X_deep_v_scaler_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:17:21.648674Z",
     "start_time": "2021-08-04T04:16:39.493915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahead_time: 0 (8406, 1, 8, 31, 31, 1)\n",
      "ahead_time: 1 (8406, 1, 8, 31, 31, 1)\n",
      "ahead_time: 2 (8406, 1, 8, 31, 31, 1)\n",
      "ahead_time: 3 (8406, 1, 8, 31, 31, 1)\n"
     ]
    }
   ],
   "source": [
    "reanalysis_type = 'u'\n",
    "for ahead_time in ahead_times:\n",
    "\n",
    "    reanalysis_list = []\n",
    "    for pressure in pressures:\n",
    "        folder = None\n",
    "        if ahead_time == 0:\n",
    "            folder = reanalysis_type\n",
    "        else:\n",
    "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
    "            \n",
    "        train_reanalysis_csv = pd.read_csv('../../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_train_31_31.csv', header=None)\n",
    "        test_reanalysis_csv = pd.read_csv('../../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_test_31_31.csv', header=None)\n",
    "\n",
    "        train_reanalysis = train_reanalysis_csv[train_reanalysis_csv[0].isin(train[0].unique())]\n",
    "        test_reanalysis = test_reanalysis_csv[test_reanalysis_csv[0].isin(test[0].unique())]\n",
    "        reanalysis_u_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
    "        \n",
    "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
    "        reanalysis.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "        X_deep_u_scaler_dict[scaler_name] = MinMaxScaler()\n",
    "        \n",
    "        # 5:end is the 31*31 u component wind speed\n",
    "        X_deep = X_deep_u_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
    "        \n",
    "         # (batch, type, channel, height, widht, time) here type is u\n",
    "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 31, 31, 1)\n",
    "        reanalysis_list.append(X_deep_final)\n",
    "        \n",
    "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
    "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
    "    sequential_reanalysis_u_list.append(X_deep_temp)\n",
    "\n",
    "X_deep_u_train = np.concatenate(sequential_reanalysis_u_list, axis=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:18:02.811143Z",
     "start_time": "2021-08-04T04:17:21.650426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahead_time: 0 (8406, 1, 8, 31, 31, 1)\n",
      "ahead_time: 1 (8406, 1, 8, 31, 31, 1)\n",
      "ahead_time: 2 (8406, 1, 8, 31, 31, 1)\n",
      "ahead_time: 3 (8406, 1, 8, 31, 31, 1)\n"
     ]
    }
   ],
   "source": [
    "reanalysis_type = 'v'\n",
    "for ahead_time in ahead_times:\n",
    "\n",
    "    reanalysis_list = []\n",
    "    for pressure in pressures:\n",
    "        folder = None\n",
    "        if ahead_time == 0:\n",
    "            folder = reanalysis_type\n",
    "        else:\n",
    "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
    "\n",
    "        train_reanalysis_csv = pd.read_csv('../../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_train_31_31.csv', header=None)\n",
    "        test_reanalysis_csv = pd.read_csv('../../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_test_31_31.csv', header=None)\n",
    "\n",
    "        train_reanalysis = train_reanalysis_csv[train_reanalysis_csv[0].isin(train[0].unique())]\n",
    "        test_reanalysis = test_reanalysis_csv[test_reanalysis_csv[0].isin(test[0].unique())]\n",
    "        reanalysis_v_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
    "\n",
    "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
    "        reanalysis.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "        X_deep_v_scaler_dict[scaler_name] = MinMaxScaler()\n",
    "        \n",
    "        # 5:end is the 31*31 u component wind speed\n",
    "        X_deep = X_deep_v_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
    "        # (batch, type, channel, height, widht, time) here type is u\n",
    "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 31, 31, 1)\n",
    "        reanalysis_list.append(X_deep_final)\n",
    "        \n",
    "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
    "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
    "    sequential_reanalysis_v_list.append(X_deep_temp)\n",
    "\n",
    "X_deep_v_train = np.concatenate(sequential_reanalysis_v_list, axis=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:18:04.009467Z",
     "start_time": "2021-08-04T04:18:02.812818Z"
    }
   },
   "outputs": [],
   "source": [
    "X_deep_train = np.concatenate((X_deep_u_train, X_deep_v_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:18:04.016379Z",
     "start_time": "2021-08-04T04:18:04.011955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8406, 96), (8406, 2, 8, 31, 31, 4))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_train.shape, X_deep_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:18:04.075299Z",
     "start_time": "2021-08-04T04:18:04.018177Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainLoader(Data.Dataset):\n",
    "    def __init__(self, X_wide_train, X_deep_train, y_train):\n",
    "        self.X_wide_train = X_wide_train\n",
    "        self.X_deep_train = X_deep_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return [self.X_wide_train[index], self.X_deep_train[index]], self.y_train[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_wide_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:18:04.161275Z",
     "start_time": "2021-08-04T04:18:04.077497Z"
    }
   },
   "outputs": [],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:18:04.219089Z",
     "start_time": "2021-08-04T04:18:04.163672Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7565, 841)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_index), len(val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:18:05.623450Z",
     "start_time": "2021-08-04T04:18:04.222588Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:18:05.772700Z",
     "start_time": "2021-08-04T04:18:05.624821Z"
    }
   },
   "outputs": [],
   "source": [
    "val_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D SAF-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:18:05.788254Z",
     "start_time": "2021-08-04T04:18:05.774478Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(2, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7 * 2, 128)\n",
    "        self.fc2 = nn.Linear(96 + 128*4, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, wide, deep):\n",
    "        seq_list = []\n",
    "        for i in range(len(ahead_times)):\n",
    "            timeseq = deep[...,i]\n",
    "            timeseq = self.pool1(F.relu(self.conv1(timeseq)))\n",
    "            timeseq = self.pool2(F.relu(self.conv2(timeseq)))\n",
    "            timeseq = timeseq.view(-1, 128 * 7 * 7 * 2)\n",
    "            timeseq = self.fc1(timeseq)\n",
    "            seq_list.append(timeseq)\n",
    "        wide = wide.view(-1, 96)\n",
    "        wide_n_deep = torch.cat((wide, seq_list[0]),1)\n",
    "        if len(ahead_times) > 1:\n",
    "            for i in range(1, len(ahead_times)):\n",
    "                wide_n_deep = torch.cat((wide_n_deep, seq_list[i]),1)\n",
    "        wide_n_deep = self.fc2(wide_n_deep)\n",
    "        wide_n_deep = self.fc3(wide_n_deep)\n",
    "        wide_n_deep = self.fc4(wide_n_deep)\n",
    "        return wide_n_deep\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:18:07.165993Z",
     "start_time": "2021-08-04T04:18:05.789363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv3d(2, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool1): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool2): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=12544, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=608, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = net.to(device)\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:18:07.169526Z",
     "start_time": "2021-08-04T04:18:07.167094Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T10:20:03.098799Z",
     "start_time": "2021-07-30T10:02:46.321382Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs [1/128] cost:8.00s train_loss: 12.40180 val_loss: 0.52767\n",
      "epochs [2/128] cost:8.00s train_loss: 4.45969 val_loss: 0.51595\n",
      "epochs [3/128] cost:8.00s train_loss: 4.33298 val_loss: 0.48625\n",
      "epochs [4/128] cost:8.00s train_loss: 4.18216 val_loss: 0.48746\n",
      "epochs [5/128] cost:8.00s train_loss: 4.20319 val_loss: 0.47350\n",
      "epochs [6/128] cost:8.00s train_loss: 3.97236 val_loss: 0.46013\n",
      "epochs [7/128] cost:8.00s train_loss: 4.03655 val_loss: 0.45484\n",
      "epochs [8/128] cost:8.00s train_loss: 3.82168 val_loss: 0.55231\n",
      "epochs [9/128] cost:8.00s train_loss: 3.87506 val_loss: 0.49459\n",
      "epochs [10/128] cost:8.00s train_loss: 4.29568 val_loss: 0.44673\n",
      "epochs [11/128] cost:8.00s train_loss: 3.80378 val_loss: 0.43510\n",
      "epochs [12/128] cost:8.00s train_loss: 3.69335 val_loss: 0.41612\n",
      "epochs [13/128] cost:8.00s train_loss: 3.81514 val_loss: 0.42895\n",
      "epochs [14/128] cost:8.00s train_loss: 3.90686 val_loss: 0.48735\n",
      "epochs [15/128] cost:8.00s train_loss: 3.75642 val_loss: 0.42843\n",
      "epochs [16/128] cost:8.00s train_loss: 3.75851 val_loss: 0.41493\n",
      "epochs [17/128] cost:8.00s train_loss: 3.68910 val_loss: 0.44955\n",
      "epochs [18/128] cost:8.00s train_loss: 3.63601 val_loss: 0.41801\n",
      "epochs [19/128] cost:8.00s train_loss: 3.62385 val_loss: 0.39860\n",
      "epochs [20/128] cost:8.00s train_loss: 3.71122 val_loss: 0.55165\n",
      "epochs [21/128] cost:8.00s train_loss: 3.71337 val_loss: 0.47555\n",
      "epochs [22/128] cost:8.00s train_loss: 3.61247 val_loss: 0.42979\n",
      "epochs [23/128] cost:8.00s train_loss: 3.58514 val_loss: 0.44843\n",
      "epochs [24/128] cost:8.00s train_loss: 3.57174 val_loss: 0.42059\n",
      "epochs [25/128] cost:8.00s train_loss: 3.45402 val_loss: 0.43144\n",
      "epochs [26/128] cost:8.00s train_loss: 3.49230 val_loss: 0.40530\n",
      "epochs [27/128] cost:8.00s train_loss: 3.52681 val_loss: 0.41425\n",
      "epochs [28/128] cost:8.00s train_loss: 3.49734 val_loss: 0.39712\n",
      "epochs [29/128] cost:8.00s train_loss: 3.47900 val_loss: 0.39712\n",
      "epochs [30/128] cost:8.00s train_loss: 3.44684 val_loss: 0.40472\n",
      "epochs [31/128] cost:8.00s train_loss: 3.44696 val_loss: 0.38505\n",
      "epochs [32/128] cost:8.00s train_loss: 3.38152 val_loss: 0.41959\n",
      "epochs [33/128] cost:8.00s train_loss: 3.39409 val_loss: 0.39913\n",
      "epochs [34/128] cost:8.00s train_loss: 3.43125 val_loss: 0.40685\n",
      "epochs [35/128] cost:8.00s train_loss: 3.36639 val_loss: 0.41537\n",
      "epochs [36/128] cost:8.00s train_loss: 3.40883 val_loss: 0.47075\n",
      "epochs [37/128] cost:8.00s train_loss: 3.48203 val_loss: 0.39603\n",
      "epochs [38/128] cost:8.00s train_loss: 3.38978 val_loss: 0.41720\n",
      "epochs [39/128] cost:8.00s train_loss: 3.36041 val_loss: 0.40846\n",
      "epochs [40/128] cost:8.00s train_loss: 3.33237 val_loss: 0.39659\n",
      "epochs [41/128] cost:8.00s train_loss: 3.38756 val_loss: 0.39061\n",
      "epochs [42/128] cost:8.00s train_loss: 3.32884 val_loss: 0.38960\n",
      "epochs [43/128] cost:8.00s train_loss: 3.30324 val_loss: 0.40040\n",
      "epochs [44/128] cost:8.00s train_loss: 3.31435 val_loss: 0.40547\n",
      "epochs [45/128] cost:8.00s train_loss: 3.41027 val_loss: 0.44391\n",
      "epochs [46/128] cost:8.00s train_loss: 3.42290 val_loss: 0.39396\n",
      "epochs [47/128] cost:8.00s train_loss: 3.30605 val_loss: 0.40303\n",
      "epochs [48/128] cost:8.00s train_loss: 3.30315 val_loss: 0.39828\n",
      "epochs [49/128] cost:8.00s train_loss: 3.31601 val_loss: 0.41361\n",
      "epochs [50/128] cost:8.00s train_loss: 3.41327 val_loss: 0.40588\n",
      "epochs [51/128] cost:8.00s train_loss: 3.24999 val_loss: 0.38430\n",
      "epochs [52/128] cost:8.00s train_loss: 3.37661 val_loss: 0.44414\n",
      "epochs [53/128] cost:8.00s train_loss: 3.34081 val_loss: 0.43932\n",
      "epochs [54/128] cost:8.00s train_loss: 3.30028 val_loss: 0.40923\n",
      "epochs [55/128] cost:8.00s train_loss: 3.21764 val_loss: 0.37849\n",
      "epochs [56/128] cost:8.00s train_loss: 3.28065 val_loss: 0.43041\n",
      "epochs [57/128] cost:8.00s train_loss: 3.22300 val_loss: 0.38988\n",
      "epochs [58/128] cost:8.00s train_loss: 3.20512 val_loss: 0.40229\n",
      "epochs [59/128] cost:8.00s train_loss: 3.31915 val_loss: 0.38019\n",
      "epochs [60/128] cost:8.00s train_loss: 3.27378 val_loss: 0.41872\n",
      "epochs [61/128] cost:8.00s train_loss: 3.31318 val_loss: 0.43979\n",
      "epochs [62/128] cost:8.00s train_loss: 3.23783 val_loss: 0.44701\n",
      "epochs [63/128] cost:8.00s train_loss: 3.33051 val_loss: 0.38152\n",
      "epochs [64/128] cost:8.00s train_loss: 3.17244 val_loss: 0.39330\n",
      "epochs [65/128] cost:8.00s train_loss: 3.29567 val_loss: 0.34340\n",
      "epochs [66/128] cost:8.00s train_loss: 3.19586 val_loss: 0.39078\n",
      "epochs [67/128] cost:8.00s train_loss: 3.14567 val_loss: 0.37436\n",
      "epochs [68/128] cost:8.00s train_loss: 3.10911 val_loss: 0.35603\n",
      "epochs [69/128] cost:8.00s train_loss: 3.16940 val_loss: 0.35184\n",
      "epochs [70/128] cost:8.00s train_loss: 3.14373 val_loss: 0.34948\n",
      "epochs [71/128] cost:8.00s train_loss: 3.20676 val_loss: 0.35672\n",
      "epochs [72/128] cost:8.00s train_loss: 3.08511 val_loss: 0.41292\n",
      "epochs [73/128] cost:8.00s train_loss: 3.11947 val_loss: 0.37911\n",
      "epochs [74/128] cost:8.00s train_loss: 3.09463 val_loss: 0.36634\n",
      "epochs [75/128] cost:8.00s train_loss: 3.11622 val_loss: 0.34951\n",
      "epochs [76/128] cost:8.00s train_loss: 3.09963 val_loss: 0.35866\n",
      "epochs [77/128] cost:8.00s train_loss: 3.10363 val_loss: 0.38170\n",
      "epochs [78/128] cost:8.00s train_loss: 3.06721 val_loss: 0.39532\n",
      "epochs [79/128] cost:8.00s train_loss: 3.07531 val_loss: 0.39973\n",
      "epochs [80/128] cost:8.00s train_loss: 3.05871 val_loss: 0.31846\n",
      "epochs [81/128] cost:8.00s train_loss: 3.09548 val_loss: 0.35541\n",
      "epochs [82/128] cost:8.00s train_loss: 3.17241 val_loss: 0.42091\n",
      "epochs [83/128] cost:8.00s train_loss: 3.05346 val_loss: 0.35224\n",
      "epochs [84/128] cost:8.00s train_loss: 3.08071 val_loss: 0.34877\n",
      "epochs [85/128] cost:8.00s train_loss: 3.03360 val_loss: 0.35099\n",
      "epochs [86/128] cost:8.00s train_loss: 3.04586 val_loss: 0.39211\n",
      "epochs [87/128] cost:8.00s train_loss: 3.04407 val_loss: 0.39443\n",
      "epochs [88/128] cost:8.00s train_loss: 3.11758 val_loss: 0.36848\n",
      "epochs [89/128] cost:8.00s train_loss: 3.00322 val_loss: 0.35585\n",
      "epochs [90/128] cost:8.00s train_loss: 3.00727 val_loss: 0.36365\n",
      "epochs [91/128] cost:8.00s train_loss: 3.06893 val_loss: 0.33274\n",
      "epochs [92/128] cost:8.00s train_loss: 2.96165 val_loss: 0.34332\n",
      "epochs [93/128] cost:8.00s train_loss: 3.05005 val_loss: 0.38011\n",
      "epochs [94/128] cost:8.00s train_loss: 3.06226 val_loss: 0.33106\n",
      "epochs [95/128] cost:8.00s train_loss: 2.96200 val_loss: 0.34957\n",
      "epochs [96/128] cost:8.00s train_loss: 3.04532 val_loss: 0.32157\n",
      "epochs [97/128] cost:8.00s train_loss: 2.97307 val_loss: 0.33256\n",
      "epochs [98/128] cost:8.00s train_loss: 2.96420 val_loss: 0.34662\n",
      "epochs [99/128] cost:8.00s train_loss: 2.96260 val_loss: 0.32979\n",
      "epochs [100/128] cost:8.00s train_loss: 2.97666 val_loss: 0.34968\n",
      "epochs [101/128] cost:8.00s train_loss: 2.92862 val_loss: 0.34503\n",
      "epochs [102/128] cost:8.00s train_loss: 2.94353 val_loss: 0.34934\n",
      "epochs [103/128] cost:8.00s train_loss: 2.92908 val_loss: 0.33764\n",
      "epochs [104/128] cost:8.00s train_loss: 2.93757 val_loss: 0.33777\n",
      "epochs [105/128] cost:8.00s train_loss: 2.89071 val_loss: 0.39883\n",
      "epochs [106/128] cost:8.00s train_loss: 3.00550 val_loss: 0.36904\n",
      "epochs [107/128] cost:8.00s train_loss: 2.89108 val_loss: 0.32336\n",
      "epochs [108/128] cost:8.00s train_loss: 2.99869 val_loss: 0.32827\n",
      "epochs [109/128] cost:8.00s train_loss: 2.89845 val_loss: 0.34827\n",
      "epochs [110/128] cost:8.00s train_loss: 2.88150 val_loss: 0.33878\n",
      "epochs [111/128] cost:8.00s train_loss: 2.86828 val_loss: 0.35720\n",
      "epochs [112/128] cost:8.00s train_loss: 2.85855 val_loss: 0.34775\n",
      "epochs [113/128] cost:8.00s train_loss: 2.86328 val_loss: 0.34409\n",
      "epochs [114/128] cost:8.00s train_loss: 2.88697 val_loss: 0.32723\n",
      "epochs [115/128] cost:8.00s train_loss: 2.86254 val_loss: 0.33525\n",
      "epochs [116/128] cost:8.00s train_loss: 2.91634 val_loss: 0.32924\n",
      "epochs [117/128] cost:8.00s train_loss: 2.89509 val_loss: 0.34602\n",
      "epochs [118/128] cost:8.00s train_loss: 2.88683 val_loss: 0.35391\n",
      "epochs [119/128] cost:8.00s train_loss: 2.94257 val_loss: 0.34036\n",
      "epochs [120/128] cost:8.00s train_loss: 2.95345 val_loss: 0.38656\n",
      "epochs [121/128] cost:8.00s train_loss: 2.87400 val_loss: 0.33408\n",
      "epochs [122/128] cost:8.00s train_loss: 2.80284 val_loss: 0.33619\n",
      "epochs [123/128] cost:8.00s train_loss: 2.77651 val_loss: 0.32217\n",
      "epochs [124/128] cost:8.00s train_loss: 2.79366 val_loss: 0.34429\n",
      "epochs [125/128] cost:8.00s train_loss: 2.84274 val_loss: 0.41356\n",
      "epochs [126/128] cost:8.00s train_loss: 2.92442 val_loss: 0.31456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs [127/128] cost:8.00s train_loss: 2.81850 val_loss: 0.31414\n",
      "epochs [128/128] cost:8.00s train_loss: 2.77843 val_loss: 0.31296\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    starttime = datetime.datetime.now()\n",
    "    train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)\n",
    "    train_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    val_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    total_train_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(train_dataset):\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "            X_wide_train_cuda = batch_x[0].float().cuda()\n",
    "            X_deep_train_cuda = batch_x[1].float().cuda()\n",
    "            y_train_cuda = batch_y.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        pred_y = net(X_wide_train_cuda, X_deep_train_cuda)\n",
    "        loss = criterion(pred_y, y_train_cuda)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    total_val_loss = 0\n",
    "    for _,(batch_val_x, batch_val_y) in enumerate(val_dataset):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_val_cuda = batch_val_x[0].float().cuda()\n",
    "            X_deep_val_cuda = batch_val_x[1].float().cuda()\n",
    "            y_val_cuda = batch_val_y.cuda()\n",
    "        \n",
    "        pred_y = net(X_wide_val_cuda, X_deep_val_cuda)\n",
    "        val_loss = criterion(pred_y, y_val_cuda)\n",
    "        total_val_loss += val_loss.item()\n",
    "    \n",
    "        # print statistics\n",
    "    if min_val_loss > total_val_loss:\n",
    "        torch.save(net.state_dict(), model_name)\n",
    "        min_val_loss = total_val_loss\n",
    "    endtime = datetime.datetime.now()\n",
    "    print('epochs [%d/%d] cost:%.2fs train_loss: %.5f val_loss: %.5f' % \n",
    "          (epoch + 1, epochs, (endtime-starttime).seconds, total_train_loss, total_val_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:18:07.261246Z",
     "start_time": "2021-08-04T04:18:07.171748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(model_name))\n",
    "\n",
    "years = test[4].unique()\n",
    "\n",
    "test_list = []\n",
    "\n",
    "for year in years:\n",
    "    temp = test[test[4]==year]\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    test_list.append(temp)\n",
    "    \n",
    "len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:18:12.525998Z",
     "start_time": "2021-08-04T04:18:07.263596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 年:\n",
      "avg wind error: [4.7979302]\n",
      "2016 年:\n",
      "avg wind error: [6.31414002]\n",
      "2017 年:\n",
      "avg wind error: [4.34779083]\n",
      "2018 年:\n",
      "avg wind error: [5.2558387]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for year, _test in zip(years, test_list):\n",
    "\n",
    "        print(year, '年:')\n",
    "        y_test = _test.loc[:,3]\n",
    "        X_wide_test = X_wide_scaler.transform(_test.loc[:,5:])\n",
    "\n",
    "        final_test_u_list = []\n",
    "        for ahead_time in ahead_times:\n",
    "            year_test_list = []\n",
    "            for pressure in pressures:\n",
    "                scaler_name = 'u' +str(pressure) + str(ahead_time)\n",
    "                X_deep = reanalysis_u_test_dict[scaler_name][reanalysis_u_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
    "                X_deep = X_deep_u_scaler_dict[scaler_name].transform(X_deep)\n",
    "                X_deep_final = X_deep.reshape(-1, 1, 1, 31, 31, 1)\n",
    "                year_test_list.append(X_deep_final)\n",
    "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
    "            final_test_u_list.append(X_deep_temp)\n",
    "        X_deep_u_test = np.concatenate(final_test_u_list, axis=5)\n",
    "        \n",
    "        final_test_v_list = []\n",
    "        for ahead_time in ahead_times:\n",
    "            year_test_list = []\n",
    "            for pressure in pressures:\n",
    "                scaler_name = 'v' +str(pressure) + str(ahead_time)\n",
    "                X_deep = reanalysis_v_test_dict[scaler_name][reanalysis_v_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
    "                X_deep = X_deep_v_scaler_dict[scaler_name].transform(X_deep)\n",
    "                X_deep_final = X_deep.reshape(-1, 1, 1, 31, 31, 1)\n",
    "                year_test_list.append(X_deep_final)\n",
    "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
    "            final_test_v_list.append(X_deep_temp)\n",
    "        X_deep_v_test = np.concatenate(final_test_v_list, axis=5)\n",
    "    \n",
    "        X_deep_test = np.concatenate((X_deep_u_test, X_deep_v_test), axis=1)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_test = Variable(torch.from_numpy(X_wide_test).float().cuda())\n",
    "            X_deep_test = Variable(torch.from_numpy(X_deep_test).float().cuda())\n",
    "\n",
    "        pred = net(X_wide_test, X_deep_test)\n",
    "\n",
    "        pred = y_scaler.inverse_transform(pred.cpu().detach().numpy().reshape(-1,1))\n",
    "        true = y_test.values.reshape(-1, 1)\n",
    "        diff = np.abs(pred - true)\n",
    "\n",
    "        print('avg wind error:', sum(diff)/len(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
