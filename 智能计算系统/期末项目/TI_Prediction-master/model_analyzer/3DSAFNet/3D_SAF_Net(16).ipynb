{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:40:20.902293Z",
     "start_time": "2021-08-04T04:40:13.085505Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:40:20.910172Z",
     "start_time": "2021-08-04T04:40:20.905503Z"
    }
   },
   "outputs": [],
   "source": [
    "#  predition TI of leading time at 24 hours\n",
    "pre_seq = 4\n",
    "batch_size = 128\n",
    "epochs = 128\n",
    "min_val_loss = 100\n",
    "model_name = '3D_SAF_Net(16).pkl'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:40:21.450119Z",
     "start_time": "2021-08-04T04:40:20.913754Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/CMA_train_'+str(pre_seq*6)+'h.csv', header=None)\n",
    "test= pd.read_csv('../../data/CMA_test_'+str(pre_seq*6)+'h.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:40:21.456121Z",
     "start_time": "2021-08-04T04:40:21.451778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8406, 101), (2747, 101))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:40:21.586639Z",
     "start_time": "2021-08-04T04:40:21.458202Z"
    }
   },
   "outputs": [],
   "source": [
    "CLIPER_feature =  pd.concat((train, test), axis=0)\n",
    "CLIPER_feature.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:40:21.723075Z",
     "start_time": "2021-08-04T04:40:21.588836Z"
    }
   },
   "outputs": [],
   "source": [
    "X_wide_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X_wide = X_wide_scaler.fit_transform(CLIPER_feature.iloc[:, 5:])\n",
    "X_wide_train = X_wide[0: train.shape[0], :]\n",
    "\n",
    "y = y_scaler.fit_transform(CLIPER_feature.loc[:, 3].values.reshape(-1,1))\n",
    "y_train = y[0: train.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:40:21.731793Z",
     "start_time": "2021-08-04T04:40:21.726082Z"
    }
   },
   "outputs": [],
   "source": [
    "# now 6 hours ago  12 hours ago  18 hour ago\n",
    "ahead_times = [0,1,2,3]\n",
    "\n",
    "pressures = [1000, 950, 900, 850, 800, 750, 700, 650, 600, 550, 500, 450, 400, 350, 300, 250]\n",
    "\n",
    "sequential_reanalysis_u_list = []\n",
    "reanalysis_u_test_dict = {}\n",
    "X_deep_u_scaler_dict = {}\n",
    "\n",
    "sequential_reanalysis_v_list = []\n",
    "reanalysis_v_test_dict = {}\n",
    "X_deep_v_scaler_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:42:28.864458Z",
     "start_time": "2021-08-04T04:40:21.735671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahead_time: 0 (8406, 1, 16, 31, 31, 1)\n",
      "ahead_time: 1 (8406, 1, 16, 31, 31, 1)\n",
      "ahead_time: 2 (8406, 1, 16, 31, 31, 1)\n",
      "ahead_time: 3 (8406, 1, 16, 31, 31, 1)\n"
     ]
    }
   ],
   "source": [
    "reanalysis_type = 'u'\n",
    "for ahead_time in ahead_times:\n",
    "\n",
    "    reanalysis_list = []\n",
    "    for pressure in pressures:\n",
    "        folder = None\n",
    "        if ahead_time == 0:\n",
    "            folder = reanalysis_type\n",
    "        else:\n",
    "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
    "            \n",
    "        train_reanalysis_csv = pd.read_csv('../../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_train_31_31.csv', header=None)\n",
    "        test_reanalysis_csv = pd.read_csv('../../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_test_31_31.csv', header=None)\n",
    "\n",
    "        train_reanalysis = train_reanalysis_csv[train_reanalysis_csv[0].isin(train[0].unique())]\n",
    "        test_reanalysis = test_reanalysis_csv[test_reanalysis_csv[0].isin(test[0].unique())]\n",
    "        reanalysis_u_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
    "        \n",
    "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
    "        reanalysis.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "        X_deep_u_scaler_dict[scaler_name] = MinMaxScaler()\n",
    "        \n",
    "        # 5:end is the 31*31 u component wind speed\n",
    "        X_deep = X_deep_u_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
    "        \n",
    "         # (batch, type, channel, height, widht, time) here type is u\n",
    "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 31, 31, 1)\n",
    "        reanalysis_list.append(X_deep_final)\n",
    "        \n",
    "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
    "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
    "    sequential_reanalysis_u_list.append(X_deep_temp)\n",
    "\n",
    "X_deep_u_train = np.concatenate(sequential_reanalysis_u_list, axis=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:44:28.773722Z",
     "start_time": "2021-08-04T04:42:28.866038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahead_time: 0 (8406, 1, 16, 31, 31, 1)\n",
      "ahead_time: 1 (8406, 1, 16, 31, 31, 1)\n",
      "ahead_time: 2 (8406, 1, 16, 31, 31, 1)\n",
      "ahead_time: 3 (8406, 1, 16, 31, 31, 1)\n"
     ]
    }
   ],
   "source": [
    "reanalysis_type = 'v'\n",
    "for ahead_time in ahead_times:\n",
    "\n",
    "    reanalysis_list = []\n",
    "    for pressure in pressures:\n",
    "        folder = None\n",
    "        if ahead_time == 0:\n",
    "            folder = reanalysis_type\n",
    "        else:\n",
    "            folder = reanalysis_type + '_' + str(ahead_time*6)\n",
    "\n",
    "        train_reanalysis_csv = pd.read_csv('../../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_train_31_31.csv', header=None)\n",
    "        test_reanalysis_csv = pd.read_csv('../../data/ERA_Interim/'+folder+'/'+reanalysis_type+str(pressure)+'_test_31_31.csv', header=None)\n",
    "\n",
    "        train_reanalysis = train_reanalysis_csv[train_reanalysis_csv[0].isin(train[0].unique())]\n",
    "        test_reanalysis = test_reanalysis_csv[test_reanalysis_csv[0].isin(test[0].unique())]\n",
    "        reanalysis_v_test_dict[reanalysis_type+str(pressure)+str(ahead_time)] = test_reanalysis # 保存test 用于后面测试\n",
    "\n",
    "        reanalysis =  pd.concat((train_reanalysis, test_reanalysis), axis=0)\n",
    "        reanalysis.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        scaler_name = reanalysis_type +str(pressure) + str(ahead_time)\n",
    "        X_deep_v_scaler_dict[scaler_name] = MinMaxScaler()\n",
    "        \n",
    "        # 5:end is the 31*31 u component wind speed\n",
    "        X_deep = X_deep_v_scaler_dict[scaler_name].fit_transform(reanalysis.loc[:, 5:])\n",
    "        # (batch, type, channel, height, widht, time) here type is u\n",
    "        X_deep_final = X_deep[0: train.shape[0], :].reshape(-1, 1, 1, 31, 31, 1)\n",
    "        reanalysis_list.append(X_deep_final)\n",
    "        \n",
    "    X_deep_temp = np.concatenate(reanalysis_list[:], axis=2)\n",
    "    print(\"ahead_time:\", ahead_time, X_deep_temp.shape)\n",
    "    sequential_reanalysis_v_list.append(X_deep_temp)\n",
    "\n",
    "X_deep_v_train = np.concatenate(sequential_reanalysis_v_list, axis=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:44:31.036017Z",
     "start_time": "2021-08-04T04:44:28.775020Z"
    }
   },
   "outputs": [],
   "source": [
    "X_deep_train = np.concatenate((X_deep_u_train, X_deep_v_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:44:31.040113Z",
     "start_time": "2021-08-04T04:44:31.037168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8406, 96), (8406, 2, 16, 31, 31, 4))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wide_train.shape, X_deep_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:44:31.107997Z",
     "start_time": "2021-08-04T04:44:31.041059Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainLoader(Data.Dataset):\n",
    "    def __init__(self, X_wide_train, X_deep_train, y_train):\n",
    "        self.X_wide_train = X_wide_train\n",
    "        self.X_deep_train = X_deep_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return [self.X_wide_train[index], self.X_deep_train[index]], self.y_train[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_wide_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:44:31.195864Z",
     "start_time": "2021-08-04T04:44:31.110359Z"
    }
   },
   "outputs": [],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:44:31.251309Z",
     "start_time": "2021-08-04T04:44:31.198698Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7565, 841)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_index), len(val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:44:33.716071Z",
     "start_time": "2021-08-04T04:44:31.253817Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:44:33.981043Z",
     "start_time": "2021-08-04T04:44:33.717683Z"
    }
   },
   "outputs": [],
   "source": [
    "val_dataset = torch.utils.data.DataLoader(\n",
    "    TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D SAF-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:44:34.269078Z",
     "start_time": "2021-08-04T04:44:33.982309Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv3d(2, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        self.conv3 = nn.Conv3d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
    "        self.fc1 = nn.Linear(256 * 3 * 3 * 2, 128)\n",
    "        self.fc2 = nn.Linear(96 + 128*4, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, wide, deep):\n",
    "        seq_list = []\n",
    "        for i in range(len(ahead_times)):\n",
    "            timeseq = deep[...,i]\n",
    "            timeseq = self.pool1(F.relu(self.conv1(timeseq)))\n",
    "            timeseq = self.pool2(F.relu(self.conv2(timeseq)))\n",
    "            timeseq = self.pool3(F.relu(self.conv3(timeseq)))\n",
    "            timeseq = timeseq.view(-1, 256 * 3 * 3 * 2)\n",
    "            timeseq = self.fc1(timeseq)\n",
    "            seq_list.append(timeseq)\n",
    "        wide = wide.view(-1, 96)\n",
    "        wide_n_deep = torch.cat((wide, seq_list[0]),1)\n",
    "        if len(ahead_times) > 1:\n",
    "            for i in range(1, len(ahead_times)):\n",
    "                wide_n_deep = torch.cat((wide_n_deep, seq_list[i]),1)\n",
    "        wide_n_deep = self.fc2(wide_n_deep)\n",
    "        wide_n_deep = self.fc3(wide_n_deep)\n",
    "        wide_n_deep = self.fc4(wide_n_deep)\n",
    "        return wide_n_deep\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:44:44.931840Z",
     "start_time": "2021-08-04T04:44:34.271057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv3d(2, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool1): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool2): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=4608, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=608, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = net.to(device)\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:44:44.939938Z",
     "start_time": "2021-08-04T04:44:44.935831Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T11:28:07.394953Z",
     "start_time": "2021-07-30T10:49:32.733433Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs [1/128] cost:22.00s train_loss: 7.10606 val_loss: 0.57337\n",
      "epochs [2/128] cost:18.00s train_loss: 4.61308 val_loss: 0.54740\n",
      "epochs [3/128] cost:18.00s train_loss: 4.29875 val_loss: 0.56283\n",
      "epochs [4/128] cost:18.00s train_loss: 4.26359 val_loss: 0.48879\n",
      "epochs [5/128] cost:18.00s train_loss: 4.19668 val_loss: 0.51046\n",
      "epochs [6/128] cost:18.00s train_loss: 4.26095 val_loss: 0.46003\n",
      "epochs [7/128] cost:17.00s train_loss: 3.91829 val_loss: 0.48324\n",
      "epochs [8/128] cost:18.00s train_loss: 4.04923 val_loss: 0.44359\n",
      "epochs [9/128] cost:18.00s train_loss: 3.88308 val_loss: 0.44232\n",
      "epochs [10/128] cost:18.00s train_loss: 3.85572 val_loss: 0.49743\n",
      "epochs [11/128] cost:18.00s train_loss: 3.72200 val_loss: 0.44873\n",
      "epochs [12/128] cost:18.00s train_loss: 3.69804 val_loss: 0.45639\n",
      "epochs [13/128] cost:18.00s train_loss: 3.75829 val_loss: 0.41248\n",
      "epochs [14/128] cost:18.00s train_loss: 3.61738 val_loss: 0.39748\n",
      "epochs [15/128] cost:18.00s train_loss: 3.50220 val_loss: 0.43355\n",
      "epochs [16/128] cost:18.00s train_loss: 3.53129 val_loss: 0.41302\n",
      "epochs [17/128] cost:18.00s train_loss: 3.46158 val_loss: 0.58077\n",
      "epochs [18/128] cost:18.00s train_loss: 3.59823 val_loss: 0.41740\n",
      "epochs [19/128] cost:18.00s train_loss: 3.47599 val_loss: 0.41639\n",
      "epochs [20/128] cost:18.00s train_loss: 3.45368 val_loss: 0.41770\n",
      "epochs [21/128] cost:18.00s train_loss: 3.36997 val_loss: 0.41877\n",
      "epochs [22/128] cost:18.00s train_loss: 3.27478 val_loss: 0.38860\n",
      "epochs [23/128] cost:18.00s train_loss: 3.37911 val_loss: 0.37966\n",
      "epochs [24/128] cost:18.00s train_loss: 3.26894 val_loss: 0.36095\n",
      "epochs [25/128] cost:18.00s train_loss: 3.19438 val_loss: 0.43378\n",
      "epochs [26/128] cost:18.00s train_loss: 3.20707 val_loss: 0.36505\n",
      "epochs [27/128] cost:18.00s train_loss: 3.15162 val_loss: 0.35404\n",
      "epochs [28/128] cost:18.00s train_loss: 3.15640 val_loss: 0.39398\n",
      "epochs [29/128] cost:18.00s train_loss: 3.22445 val_loss: 0.35634\n",
      "epochs [30/128] cost:18.00s train_loss: 3.08076 val_loss: 0.33902\n",
      "epochs [31/128] cost:18.00s train_loss: 3.04796 val_loss: 0.36122\n",
      "epochs [32/128] cost:18.00s train_loss: 3.09065 val_loss: 0.34394\n",
      "epochs [33/128] cost:18.00s train_loss: 3.03030 val_loss: 0.32440\n",
      "epochs [34/128] cost:18.00s train_loss: 2.92510 val_loss: 0.39117\n",
      "epochs [35/128] cost:18.00s train_loss: 3.02836 val_loss: 0.33787\n",
      "epochs [36/128] cost:18.00s train_loss: 2.85848 val_loss: 0.36788\n",
      "epochs [37/128] cost:18.00s train_loss: 2.85241 val_loss: 0.43505\n",
      "epochs [38/128] cost:18.00s train_loss: 2.94106 val_loss: 0.32231\n",
      "epochs [39/128] cost:18.00s train_loss: 2.93809 val_loss: 0.33954\n",
      "epochs [40/128] cost:18.00s train_loss: 2.80730 val_loss: 0.33038\n",
      "epochs [41/128] cost:18.00s train_loss: 2.80385 val_loss: 0.43247\n",
      "epochs [42/128] cost:18.00s train_loss: 2.89667 val_loss: 0.38057\n",
      "epochs [43/128] cost:18.00s train_loss: 2.76219 val_loss: 0.35841\n",
      "epochs [44/128] cost:18.00s train_loss: 2.74404 val_loss: 0.31464\n",
      "epochs [45/128] cost:18.00s train_loss: 2.64871 val_loss: 0.29888\n",
      "epochs [46/128] cost:18.00s train_loss: 2.67599 val_loss: 0.28387\n",
      "epochs [47/128] cost:18.00s train_loss: 2.49284 val_loss: 0.30541\n",
      "epochs [48/128] cost:18.00s train_loss: 2.57798 val_loss: 0.31792\n",
      "epochs [49/128] cost:18.00s train_loss: 2.55951 val_loss: 0.28509\n",
      "epochs [50/128] cost:17.00s train_loss: 2.54439 val_loss: 0.29409\n",
      "epochs [51/128] cost:17.00s train_loss: 2.51599 val_loss: 0.29453\n",
      "epochs [52/128] cost:18.00s train_loss: 2.49322 val_loss: 0.31771\n",
      "epochs [53/128] cost:18.00s train_loss: 2.45646 val_loss: 0.28020\n",
      "epochs [54/128] cost:18.00s train_loss: 2.38864 val_loss: 0.33696\n",
      "epochs [55/128] cost:18.00s train_loss: 2.52980 val_loss: 0.28969\n",
      "epochs [56/128] cost:18.00s train_loss: 2.41224 val_loss: 0.30509\n",
      "epochs [57/128] cost:18.00s train_loss: 2.42635 val_loss: 0.27015\n",
      "epochs [58/128] cost:18.00s train_loss: 2.38565 val_loss: 0.26162\n",
      "epochs [59/128] cost:18.00s train_loss: 2.41504 val_loss: 0.27371\n",
      "epochs [60/128] cost:18.00s train_loss: 2.37012 val_loss: 0.28648\n",
      "epochs [61/128] cost:18.00s train_loss: 2.35492 val_loss: 0.37926\n",
      "epochs [62/128] cost:18.00s train_loss: 2.36756 val_loss: 0.27507\n",
      "epochs [63/128] cost:18.00s train_loss: 2.34835 val_loss: 0.26870\n",
      "epochs [64/128] cost:18.00s train_loss: 2.35069 val_loss: 0.32864\n",
      "epochs [65/128] cost:18.00s train_loss: 2.29416 val_loss: 0.29947\n",
      "epochs [66/128] cost:18.00s train_loss: 2.27729 val_loss: 0.24501\n",
      "epochs [67/128] cost:18.00s train_loss: 2.20906 val_loss: 0.29110\n",
      "epochs [68/128] cost:18.00s train_loss: 2.25911 val_loss: 0.26448\n",
      "epochs [69/128] cost:18.00s train_loss: 2.22716 val_loss: 0.27315\n",
      "epochs [70/128] cost:18.00s train_loss: 2.29530 val_loss: 0.25599\n",
      "epochs [71/128] cost:18.00s train_loss: 2.19887 val_loss: 0.24012\n",
      "epochs [72/128] cost:18.00s train_loss: 2.15810 val_loss: 0.29229\n",
      "epochs [73/128] cost:18.00s train_loss: 2.16036 val_loss: 0.28426\n",
      "epochs [74/128] cost:18.00s train_loss: 2.19739 val_loss: 0.24131\n",
      "epochs [75/128] cost:18.00s train_loss: 2.12507 val_loss: 0.27309\n",
      "epochs [76/128] cost:18.00s train_loss: 2.06060 val_loss: 0.27366\n",
      "epochs [77/128] cost:18.00s train_loss: 2.15721 val_loss: 0.31838\n",
      "epochs [78/128] cost:18.00s train_loss: 2.13033 val_loss: 0.26953\n",
      "epochs [79/128] cost:18.00s train_loss: 2.13206 val_loss: 0.22724\n",
      "epochs [80/128] cost:18.00s train_loss: 2.04262 val_loss: 0.25401\n",
      "epochs [81/128] cost:18.00s train_loss: 2.09525 val_loss: 0.22777\n",
      "epochs [82/128] cost:18.00s train_loss: 2.05513 val_loss: 0.23626\n",
      "epochs [83/128] cost:18.00s train_loss: 2.13105 val_loss: 0.22354\n",
      "epochs [84/128] cost:17.00s train_loss: 2.02233 val_loss: 0.22721\n",
      "epochs [85/128] cost:18.00s train_loss: 2.00279 val_loss: 0.29932\n",
      "epochs [86/128] cost:18.00s train_loss: 2.03906 val_loss: 0.24361\n",
      "epochs [87/128] cost:18.00s train_loss: 2.02855 val_loss: 0.24034\n",
      "epochs [88/128] cost:18.00s train_loss: 1.95982 val_loss: 0.22561\n",
      "epochs [89/128] cost:18.00s train_loss: 2.04367 val_loss: 0.27900\n",
      "epochs [90/128] cost:18.00s train_loss: 2.04287 val_loss: 0.22063\n",
      "epochs [91/128] cost:18.00s train_loss: 1.97873 val_loss: 0.27135\n",
      "epochs [92/128] cost:18.00s train_loss: 2.03764 val_loss: 0.22830\n",
      "epochs [93/128] cost:18.00s train_loss: 1.97210 val_loss: 0.22065\n",
      "epochs [94/128] cost:18.00s train_loss: 1.96201 val_loss: 0.24187\n",
      "epochs [95/128] cost:18.00s train_loss: 1.94257 val_loss: 0.21095\n",
      "epochs [96/128] cost:18.00s train_loss: 1.85869 val_loss: 0.20352\n",
      "epochs [97/128] cost:18.00s train_loss: 1.86448 val_loss: 0.20883\n",
      "epochs [98/128] cost:17.00s train_loss: 1.88632 val_loss: 0.25876\n",
      "epochs [99/128] cost:17.00s train_loss: 1.82536 val_loss: 0.22819\n",
      "epochs [100/128] cost:17.00s train_loss: 1.87678 val_loss: 0.20953\n",
      "epochs [101/128] cost:18.00s train_loss: 1.81490 val_loss: 0.20908\n",
      "epochs [102/128] cost:17.00s train_loss: 1.85617 val_loss: 0.22517\n",
      "epochs [103/128] cost:18.00s train_loss: 1.93484 val_loss: 0.21089\n",
      "epochs [104/128] cost:17.00s train_loss: 1.88388 val_loss: 0.28109\n",
      "epochs [105/128] cost:17.00s train_loss: 1.98401 val_loss: 0.24098\n",
      "epochs [106/128] cost:18.00s train_loss: 1.81938 val_loss: 0.21714\n",
      "epochs [107/128] cost:18.00s train_loss: 1.77710 val_loss: 0.22866\n",
      "epochs [108/128] cost:17.00s train_loss: 1.76312 val_loss: 0.22501\n",
      "epochs [109/128] cost:18.00s train_loss: 1.82082 val_loss: 0.19153\n",
      "epochs [110/128] cost:18.00s train_loss: 1.90205 val_loss: 0.20753\n",
      "epochs [111/128] cost:18.00s train_loss: 1.73386 val_loss: 0.21528\n",
      "epochs [112/128] cost:18.00s train_loss: 1.77036 val_loss: 0.20150\n",
      "epochs [113/128] cost:18.00s train_loss: 1.75699 val_loss: 0.20304\n",
      "epochs [114/128] cost:17.00s train_loss: 1.70709 val_loss: 0.21783\n",
      "epochs [115/128] cost:17.00s train_loss: 1.72432 val_loss: 0.19925\n",
      "epochs [116/128] cost:17.00s train_loss: 1.75290 val_loss: 0.21912\n",
      "epochs [117/128] cost:17.00s train_loss: 1.75885 val_loss: 0.21319\n",
      "epochs [118/128] cost:18.00s train_loss: 1.79327 val_loss: 0.18972\n",
      "epochs [119/128] cost:17.00s train_loss: 1.63665 val_loss: 0.22998\n",
      "epochs [120/128] cost:17.00s train_loss: 1.81055 val_loss: 0.19770\n",
      "epochs [121/128] cost:18.00s train_loss: 1.65455 val_loss: 0.18826\n",
      "epochs [122/128] cost:17.00s train_loss: 1.67380 val_loss: 0.19759\n",
      "epochs [123/128] cost:18.00s train_loss: 1.70696 val_loss: 0.18703\n",
      "epochs [124/128] cost:17.00s train_loss: 1.68285 val_loss: 0.24124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs [125/128] cost:18.00s train_loss: 1.73213 val_loss: 0.21220\n",
      "epochs [126/128] cost:17.00s train_loss: 1.76024 val_loss: 0.20218\n",
      "epochs [127/128] cost:17.00s train_loss: 1.63722 val_loss: 0.22214\n",
      "epochs [128/128] cost:17.00s train_loss: 1.71366 val_loss: 0.20504\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "full_train_index = [*range(0, len(X_wide_train))]\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    starttime = datetime.datetime.now()\n",
    "    train_index, val_index, _, _, = train_test_split(full_train_index,full_train_index,test_size=0.1)\n",
    "    train_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[train_index], X_deep_train[train_index], y_train[train_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    val_dataset = torch.utils.data.DataLoader(\n",
    "        TrainLoader(X_wide_train[val_index], X_deep_train[val_index], y_train[val_index]), \n",
    "                                                 batch_size=batch_size,)\n",
    "    total_train_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(train_dataset):\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "            X_wide_train_cuda = batch_x[0].float().cuda()\n",
    "            X_deep_train_cuda = batch_x[1].float().cuda()\n",
    "            y_train_cuda = batch_y.cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        pred_y = net(X_wide_train_cuda, X_deep_train_cuda)\n",
    "        loss = criterion(pred_y, y_train_cuda)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    total_val_loss = 0\n",
    "    for _,(batch_val_x, batch_val_y) in enumerate(val_dataset):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_val_cuda = batch_val_x[0].float().cuda()\n",
    "            X_deep_val_cuda = batch_val_x[1].float().cuda()\n",
    "            y_val_cuda = batch_val_y.cuda()\n",
    "        \n",
    "        pred_y = net(X_wide_val_cuda, X_deep_val_cuda)\n",
    "        val_loss = criterion(pred_y, y_val_cuda)\n",
    "        total_val_loss += val_loss.item()\n",
    "    \n",
    "        # print statistics\n",
    "    if min_val_loss > total_val_loss:\n",
    "        torch.save(net.state_dict(), model_name)\n",
    "        min_val_loss = total_val_loss\n",
    "    endtime = datetime.datetime.now()\n",
    "    print('epochs [%d/%d] cost:%.2fs train_loss: %.5f val_loss: %.5f' % \n",
    "          (epoch + 1, epochs, (endtime-starttime).seconds, total_train_loss, total_val_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:44:45.282361Z",
     "start_time": "2021-08-04T04:44:44.942537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(model_name))\n",
    "\n",
    "years = test[4].unique()\n",
    "\n",
    "test_list = []\n",
    "\n",
    "for year in years:\n",
    "    temp = test[test[4]==year]\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    test_list.append(temp)\n",
    "    \n",
    "len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T04:44:59.347803Z",
     "start_time": "2021-08-04T04:44:45.284889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 年:\n",
      "avg wind error: [6.12687123]\n",
      "2016 年:\n",
      "avg wind error: [6.34665293]\n",
      "2017 年:\n",
      "avg wind error: [5.42323216]\n",
      "2018 年:\n",
      "avg wind error: [5.67752014]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for year, _test in zip(years, test_list):\n",
    "\n",
    "        print(year, '年:')\n",
    "        y_test = _test.loc[:,3]\n",
    "        X_wide_test = X_wide_scaler.transform(_test.loc[:,5:])\n",
    "\n",
    "        final_test_u_list = []\n",
    "        for ahead_time in ahead_times:\n",
    "            year_test_list = []\n",
    "            for pressure in pressures:\n",
    "                scaler_name = 'u' +str(pressure) + str(ahead_time)\n",
    "                X_deep = reanalysis_u_test_dict[scaler_name][reanalysis_u_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
    "                X_deep = X_deep_u_scaler_dict[scaler_name].transform(X_deep)\n",
    "                X_deep_final = X_deep.reshape(-1, 1, 1, 31, 31, 1)\n",
    "                year_test_list.append(X_deep_final)\n",
    "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
    "            final_test_u_list.append(X_deep_temp)\n",
    "        X_deep_u_test = np.concatenate(final_test_u_list, axis=5)\n",
    "        \n",
    "        final_test_v_list = []\n",
    "        for ahead_time in ahead_times:\n",
    "            year_test_list = []\n",
    "            for pressure in pressures:\n",
    "                scaler_name = 'v' +str(pressure) + str(ahead_time)\n",
    "                X_deep = reanalysis_v_test_dict[scaler_name][reanalysis_v_test_dict[scaler_name][0].isin(_test[0].unique())].loc[:,5:]\n",
    "                X_deep = X_deep_v_scaler_dict[scaler_name].transform(X_deep)\n",
    "                X_deep_final = X_deep.reshape(-1, 1, 1, 31, 31, 1)\n",
    "                year_test_list.append(X_deep_final)\n",
    "            X_deep_temp = np.concatenate(year_test_list, axis=2)\n",
    "            final_test_v_list.append(X_deep_temp)\n",
    "        X_deep_v_test = np.concatenate(final_test_v_list, axis=5)\n",
    "    \n",
    "        X_deep_test = np.concatenate((X_deep_u_test, X_deep_v_test), axis=1)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            X_wide_test = Variable(torch.from_numpy(X_wide_test).float().cuda())\n",
    "            X_deep_test = Variable(torch.from_numpy(X_deep_test).float().cuda())\n",
    "\n",
    "        pred = net(X_wide_test, X_deep_test)\n",
    "\n",
    "        pred = y_scaler.inverse_transform(pred.cpu().detach().numpy().reshape(-1,1))\n",
    "        true = y_test.values.reshape(-1, 1)\n",
    "        diff = np.abs(pred - true)\n",
    "\n",
    "        print('avg wind error:', sum(diff)/len(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
